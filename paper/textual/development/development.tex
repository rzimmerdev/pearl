%% USPSC-Cap2-Desenvolvimento.tex 

% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---

Asynchronous Shared Environments with State Consistency

Asynchronous: Environments and Learners are decoupled, allowing for independent operation,
with message queues acting as intermediate buffers for data exchange and synchronization.

Shared: Multiple workers interact with a single environment instance, reducing redundant computations and memory overhead.

State Consistency: Workers can access states previously observed by other workers, ensuring a synchronized view of the environment
if a learner was unable to process the latest state in time for synchronization.

This approach aims to improve computational efficiency and learning performance while maintaining training stability
by allowing all agents to interact synchronously on all states due to the proposed consistent environment state design.


\chapter{Development}
\label{ch:development}
%Este capítulo é parte principal do trabalho acadêmico e deve conter a exposição ordenada e detalhada do assunto.
%Divide-se em seções e subseções, em conformidade com a abordagem do tema e do método, abrangendo:
%revisão bibliográfica, materiais e métodos, técnicas utilizadas, resultados obtidos e discussão.

Distributed reinforcement learning (RL) has evolved considerably in response to the demands of complex tasks such as game playing and financial trading.
Early methods such as the Asynchronous Advantage Actor-Critic (A3C)
algorithm introduced the concept of multiple parallel workers collecting experiences independently,
which helped to reduce update variance and speed up training.
Building on these ideas, architectures like IMPALA have decoupled data collection from model training by deploying a
centralized learner that periodically synchronizes its policy with a large number of worker agents.
This design enables significant scalability across heterogeneous computing resources and facilitates high-throughput training.

A key challenge addressed by these systems is the communication overhead incurred when transmitting experiences and
updated policies between the learner and the workers.
For example, IMPALA aggregates trajectories from multiple workers into
batches for efficient gradient computation while managing the staleness of policy information.
More recent approaches, such as SEED RL, have further optimized communication by bundling inference with training updates.
This reduces the latency associated with model parameter transmission and minimizes network congestion, thereby improving overall training efficiency.
In parallel, methods like Ape-X have introduced distributed prioritized experience replay, where critical experiences are sampled with higher probability,
enhancing data efficiency in scenarios where high-frequency data generation is crucial.

In the context of financial applications, distributed RL has inspired specialized solutions that integrate realistic market simulators.
These simulators often feature a limit order book (LOB) mechanism where buy and sell orders are submitted, queued, and matched in real time.
In such systems, the distributed architecture must handle not only the high throughput of simulation data but also the
precise timing of order execution and asynchronous policy updates.
Recent studies have demonstrated that merging shared environment simulations with asynchronous updates can improve both
computational efficiency and learning performance in trading scenarios.

Despite these advances, challenges persist—such as managing synchronization delays, mitigating data staleness,
and coping with non-stationarity—especially in environments where rapid decision making is essential.
The trade-offs between synchronous and asynchronous communication, efficient use of replay buffers, and resource heterogeneity remain open research questions.
This thesis builds upon these existing approaches by proposing a novel shared parallel environment architecture,
designed specifically to address these challenges in high-frequency trading applications.
In the following sections, we present the design and implementation of our proposed architecture,
which we refer to as the Parallel Environments for Asynchronous Reinforcement Learning (PEARL).

We start with an overview of related works in distributed RL and financial trading, followed by a detailed description of the PEARL architecture.

\input{textual/development/literature}


\section{Methodology}
\label{sec:methodology}

This thesis makes several key contributions to the field of distributed reinforcement learning by introducing a novel shared
parallel environment architecture that addresses common inefficiencies in traditional systems.
Our approach, called Parallel Environments for Asynchronous Reinforcement Learning (PEARL),
is designed to enhance computational efficiency and learning performance in high-frequency trading applications.

First, the proposed framework enables multiple RL agents (workers) to share a single environment instance rather than running isolated simulations.
This design minimizes redundant computations and significantly reduces the communication overhead typically
encountered when synchronizing separate environment instances.
Second, the system incorporates an asynchronous policy update mechanism.
In our approach, workers continuously send their experience trajectories to a centralized learner without waiting for all processes to complete,
thereby mitigating issues of data staleness and enhancing convergence stability in non-stationary environments

Third, we validate the proposed architecture within the context of financial applications,
such as high-frequency trading, demonstrating that the shared environment
model can achieve superior computational efficiency and learning performance compared to conventional actor-learner designs.

\subsection{Simulated Trading Environment and Limit Order Book Dynamics}

The simulated environment is designed to replicate key aspects of a modern financial market by incorporating a limit order book (LOB) mechanism.
A LOB is the core structure in many electronic trading systems, where buy and sell orders are organized by price and time priority,
thereby reflecting the real-time supply and demand of an asset.

In this environment, agents interact with the market by submitting orders that represent their actions.
Specifically, each action corresponds to placing a limit order—a directive to buy or sell a specified quantity at a predetermined price.
When an agent submits an order, it is added to the appropriate side of the LOB, where orders are queued according to their price and
the time at which they were submitted.
An order is executed if there is a matching order on the opposite side of the book that satisfies the price condition;
otherwise, it remains in the book until a suitable counter-order appears or the order is cancelled by the agent.
This event-driven process mimics the intricate dynamics of order matching observed in real markets.

The simulation employs a discrete-event framework to model these processes accurately.
At each simulation step, the environment generates an observation that includes critical details such as the best bid and ask prices,
the order book depth across various price levels, and records of recent trade executions.
These observations are then provided to the agents, enabling them to update their strategies based on the evolving market state.

Order submission and execution in the simulated environment are handled asynchronously.
Agents send their orders via a messaging layer—implemented with a high-performance protocol like
ZeroMQ—to ensure that multiple agents can interact with the shared LOB concurrently without incurring significant delays.
This asynchronous architecture not only improves simulation throughput but also closely aligns with
the latency-sensitive nature of high-frequency trading environments.

By integrating a realistic LOB into the simulation, the environment provides a robust testbed for exploring how
distributed reinforcement learning algorithms can learn and adapt in complex, dynamic markets.
The detailed representation of market microstructure, combined with the precise handling of order events,
allows for rigorous scientific investigation into the interplay between order submission strategies and market behavior.

\subsection{Parallel Environments for Asynchronous Reinforcement Learning (PEARL)}

The proposed architecture builds on the core idea of sharing a single, highly parallel environment among multiple agents while
maintaining an efficient and scalable actor–learner framework.
In contrast to conventional distributed reinforcement learning systems—where each worker simulates an independent environment instance—
the shared environment paradigm leverages a unified simulation that is concurrently accessed by multiple workers.
This design minimizes redundant computations and memory overhead,
which is especially critical in high-frequency trading simulations where the fidelity and speed of the limit order book (LOB) dynamics are paramount.

At the heart of the architecture lies a central learner that manages the global policy.
Upon initialization, the learner spawns several worker processes using a framework such as Ray.
Each worker retrieves the latest version of the shared policy and uses it to interact with the common environment.
In this setup, workers act as both data collectors and order executors: they send orders (actions) to the LOB simulation,
receive market updates, and construct trajectories that capture the sequence of states, actions, and rewards.
By sharing the environment, workers benefit from a consistent and synchronized view of the simulated market,
which enhances the quality of the experience data collected.

Communication between the learner and workers is designed to be asynchronous.
Workers submit their generated trajectories to the learner via a high-performance messaging layer implemented with ZeroMQ.
This asynchronous mechanism enables workers to operate independently—without waiting for policy updates—
thereby reducing idle time and mitigating the staleness issues often encountered in distributed setups

). Meanwhile, the learner continuously processes incoming trajectories from the workers, updates the global policy using reinforcement learning algorithms,
and periodically broadcasts the refined policy back to the workers.
This cycle ensures that while each worker may temporarily operate on slightly outdated policy parameters,
the overall system converges steadily as fresh data are incorporated into the learner's updates.

The integration of Ray not only facilitates parallel process management but also supports dynamic resource allocation.
By abstracting the complexities of distributed computing, Ray allows the system to scale horizontally across multiple nodes,
ensuring that the computational load—especially during intensive simulations and gradient computations—is balanced effectively.
This scalability is crucial when simulating environments with rapidly changing market conditions,
where the ability to quickly process large volumes of data directly translates into more effective learning

Furthermore, the unified communication protocol and shared environment design contribute to reducing the overall system latency.
In traditional architectures, the cost of transferring model parameters and simulation data between isolated environment instances can
introduce significant delays. By contrast, our approach minimizes such overhead by centralizing the simulation and leveraging efficient,
low-latency communication channels.
This architecture is particularly well-suited for real-time applications, such as high-frequency trading,
where even minor delays can adversely affect performance

In summary, this integrated system architecture—comprising a shared simulation environment, an asynchronous actor–learner framework, and
a robust communication backbone—addresses many of the limitations inherent in earlier distributed reinforcement learning approaches.
By combining the advantages of shared resource utilization with scalable, asynchronous updates, the architecture provides a solid foundation for
developing reinforcement learning systems capable of operating effectively in complex, dynamic domains.


\section{Technical Details and Framework Description}
\label{sec:technical_details}


%\begin{table}[H]
%\begin{table}[htb]
%	\IBGEtab{%
%		\caption{Frequência anual por categoria de usuários}%
%		\label{tab-ibge}
%	}{%
%		\begin{tabular}{ccc}
%			\toprule
%			Categoria de Usuários & Frequência de Usuários \\
%			\midrule \midrule
%			Graduação & 72\% \\
%			\midrule
%			Pós-Graduação & 15\% \\
%			\midrule
%			Docente & 10\% \\
%			\midrule
%			Outras & 3\% \\
%			\bottomrule
%		\end{tabular}%
%	}{%
%		\fonte{Elaborada pelos autores.}%
%		\nota{Exemplo de uma nota.}%
%		\nota[Anotações]{Uma anotação adicional, que pode ser seguida de várias
%			outras.}%
%
%	}
%\end{table}

%
%A formatação do quadro é similar à tabela, mas deve ter suas laterais fechadas e conter as linhas horizontais.
%\newpage

% o comando \newpage foi utilizado para forçar a quebra de página

%\begin{quadro}[htb]
%	\caption{\label{quadro_modelo}Níveis de investigação}
%	\begin{tabular}{|p{2.6cm}|p{6.0cm}|p{2.25cm}|p{3.40cm}|}
%		\hline
%		\textbf{Nível de Investigação} & \textbf{Insumos}  & \textbf{Sistemas de Investigação}  & \textbf{Produtos}  \\
%		\hline
%		Meta-nível & Filosofia\index{filosofia} da Ciência  & Epistemologia &
%		Paradigma  \\
%		\hline
%		Nível do objeto & Paradigmas do metanível e evidências do nível inferior &
%		Ciência  & Teorias e modelos \\
%		\hline
%		Nível inferior & Modelos e métodos do nível do objeto e problemas do nível inferior & Prática & Solução de problemas \\
%		\hline
%	\end{tabular}
%	\begin{flushleft}
%		%\fonte{\citeonline{van1986}}
%		Fonte: \citeonline{van1986}
%	\end{flushleft}
%\end{quadro}


%% ---
%\subsection{Figuras}\label{sec_figuras}
%% ---
%\index{figuras}Figuras podem ser criadas diretamente em \LaTeX,
%como o exemplo da \autoref{fig_circulo}.

%\begin{figure}[htb]
%	\caption{\label{fig_circulo}A delimitação do espaço}
%	\begin{center}
%		\setlength{\unitlength}{9cm}
%		\begin{picture}(1,1)
%		\put(0,0){\line(0,1){1}}
%		\put(0,0){\line(1,0){1}}
%		\put(0,0){\line(1,1){1}}
%		\put(0,0){\line(1,2){.5}}
%		\put(0,0){\line(1,3){.3333}}
%		\put(0,0){\line(1,4){.25}}
%		\put(0,0){\line(1,5){.2}}
%		\put(0,0){\line(1,6){.1667}}
%		\put(0,0){\line(2,1){1}}
%		\put(0,0){\line(2,3){.6667}}
%		\put(0,0){\line(2,5){.4}}
%		\put(0,0){\line(3,1){1}}
%		\put(0,0){\line(3,2){1}}
%		\put(0,0){\line(3,4){.75}}
%		\put(0,0){\line(3,5){.6}}
%		\put(0,0){\line(4,1){1}}
%		\put(0,0){\line(4,3){1}}
%		\put(0,0){\line(4,5){.8}}
%		\put(0,0){\line(5,1){1}}
%		\put(0,0){\line(5,2){1}}
%		\put(0,0){\line(5,3){1}}
%		\put(0,0){\line(5,4){1}}
%		\put(0,0){\line(5,6){.8333}}
%		\put(0,0){\line(6,1){1}}
%		\put(0,0){\line(6,5){1}}
%		\end{picture}
%	\end{center}
%	\legend{Fonte: \citeonline{equipeabntex2}}
%\end{figure}

%Isto é uma \texttt{subsubsection} do \LaTeX, mas é denominada de ``subseção'' porque no português não temos a palavra ``subsubseção''.

%\paragraph{Este é um parágrafo numerado}\label{sec-exemplo-paragrafo}

%\begin{verbatim}
%\paragraph{Este é um parágrafo numerado}\label{sec-exemplo-paragrafo}
%\end{verbatim}







