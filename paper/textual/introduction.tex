%% USPSC-Introducao.tex

% ----------------------------------------------------------
% Introdução (exemplo de capítulo sem numeração, mas presente no Sumário)
% ----------------------------------------------------------
\chapter[Introduction]{Introdução}
\label{ch:introduction}

\section{Motivation}
\label{sec:motivation}

The field of reinforcement learning (RL) has evolved dramatically over the past decade,
driven largely by successes in domains ranging from game playing to robotics and, more recently, financial trading.
At its core, RL concerns itself with how an agent learns to make decisions by interacting with an environment to maximize cumulative rewards.
Traditionally, RL research focused on single-agent settings;
however, as problems grew in complexity and scale, the need to distribute learning and data collection became paramount.
This has led to the development of distributed RL architectures that parallelize both the simulation of environments and the learning process itself.

In conventional distributed RL frameworks, multiple agents (or actors) operate independently (each typically interacting with
its own environment instance) and report their experiences to a central learner.
Frameworks such as IMPALA and Ray RLlib have popularized this actor-learner paradigm,
allowing systems to scale by decoupling the data collection (often executed on CPUs) from the intensive training phase (executed on GPUs).
Although these approaches have advanced the state of the art, they come with inherent inefficiencies.
In many applications, particularly in finance where market dynamics require rapid decision-making,
the overhead of maintaining entirely separate environment instances for each agent can lead to redundant computations and increased latency.


This thesis explores an alternative design: a shared parallel environment architecture for reinforcement learning.
In the proposed system, multiple agents, implemented as workers, are created by a central learner.
Instead of each worker operating an isolated instance of the environment, several workers share a common environment,
thereby leveraging the parallelism inherent in modern multi-core and distributed computing platforms.
By having a shared environment, the system not only maximizes the utilization of available
computational resources but also reduces duplication of effort in simulating similar dynamics across separate instances.
This is especially valuable in domains like high-frequency trading where simulation speed directly impacts model performance and training turnaround.


The architecture is built upon modern distributed frameworks such as Ray, which provides a robust runtime
for deploying multiple actors across heterogeneous clusters.
Ray's multi-agent support ensures that workers can be efficiently created, managed, and synchronized.
Communication between the learner and workers is handled asynchronously via ZeroMQ integration, minimizing messaging
overhead and allowing each worker to send its actions and trajectory data independently.
As soon as a worker completes a trajectory, the learner is updated, while other workers continue processing without interruption.
This asynchronous design not only improves overall throughput but also enables scaling the learner and the environment independently,
an aspect critical to handling real-time financial data streams.


From a practical perspective, distributed RL has been instrumental in tackling problems that were once computationally intractable.
For example, recent research in deep RL for trading has highlighted how agents can be trained to adapt to the dynamic
and noisy environment of financial markets.
By parallelizing environment interactions and policy updates, these systems achieve better exploration
and faster convergence compared to traditional single-threaded approaches.
Moreover, the distributed setup not only addresses computational bottlenecks
but also contributes to more robust policy learning by aggregating diverse trajectories collected from different parts of the state space.
In a financial context, this means that the RL agent can better capture the myriad subtle patterns and anomalies that drive market movements.


Another important aspect motivating this work is the need for resource efficiency.
In standard distributed RL models, the replication of environments across agents leads to considerable overhead in terms of both memory and computational load.
By allowing multiple agents to share a single environment, the proposed architecture dramatically reduces these redundancies.
This design is particularly attractive for algorithmic trading applications where environments
simulate market conditions and require substantial processing power.
In such cases, sharing an environment can reduce latency and cost while maintaining, or even improving,
the quality of the experience data that feeds into the learning process.


Furthermore, asynchronous updates form a critical part of the motivation behind the proposed architecture.
In traditional synchronous RL systems, delays or bottlenecks in one worker can stall the entire training loop.
In contrast, our design decouples worker execution so that the learner continuously receives data from workers that finish their trajectories,
without waiting for all workers to synchronize.
This not only minimizes idle time but also allows the learner to update the shared policy more frequently and efficiently.
Such continuous policy updates are crucial in environments where conditions change rapidly, as is often the case in
high-frequency trading and other financial applications.


Finally, the proposed architecture addresses scalability concerns inherent to many modern RL applications.
As the number of workers increases, the traditional model suffers from communication bottlenecks and synchronization issues.
However, by designing a system where each worker communicates asynchronously with the learner through lightweight message passing protocols (e.g., ZeroMQ),
it becomes feasible to scale out to a large number of workers without a proportional increase in communication overhead.
This scalability is essential for distributed RL in finance, where large-scale data and rapid decision-making are the norms.
In such cases, being able to independently scale the processing of the environment and the learning components can result in significant performance gains.


In summary, the motivation for this research stems from the need to overcome the inefficiencies of
traditional distributed RL systems in high-stakes, real-time applications such as financial trading.
By rethinking the standard paradigm, specifically through shared parallel environments and asynchronous policy
updates, this work aims to provide an architecture that is both computationally efficient and capable of robust, scalable learning.
This approach promises to deliver faster data processing, lower latency in policy updates, and a more efficient use of computational resources,
all of which are critical in domains where time and accuracy are of the essence.

\section{Challenges in Distributed Reinforcement Learning}
\label{sec:challenges}

While distributed reinforcement learning (RL) has enabled breakthroughs in complex decision-making tasks by
leveraging parallel data collection and high-throughput training,
several challenges persist when scaling such systems for real-world applications.
One major challenge is the communication overhead between multiple workers (or actors) and a centralized learner.
In many distributed architectures, synchronizing policy updates across a large number of workers incurs significant latency.
Synchronous approaches, for example, can suffer from the ``straggler problem'' where the overall update is delayed by slower workers,
while asynchronous methods often risk data staleness due to delayed propagation of updated policy parameters.


Another challenge is ensuring consistency in the policy being executed across different agents.
In asynchronous systems, individual workers may interact with the environment using outdated versions of the shared policy,
which can lead to unstable learning dynamics.
This discrepancy between the current global policy and the locally used version in workers complicates the convergence of the learning algorithm.
Scientific studies have shown that such staleness can significantly impact performance,
particularly in environments where decisions must be made rapidly and under uncertainty.


Scalability itself presents additional difficulties.
As the number of workers increases, the system must manage not only the increased volume of experience data but
also the heterogeneity in computational resources.
Efficient resource management is critical in settings where workers operate on different hardware configurations.
The learner must handle variable data throughput while ensuring that the prioritization of experiences
(as seen in techniques like distributed prioritized experience replay) does not introduce further bottlenecks.
Moreover, optimizing the load balance between data collection and model training is essential to
prevent scenarios where either component becomes a performance bottleneck.

Non-stationarity is an inherent issue in distributed RL systems.
The dynamic nature of market environments are usually made up of multiple non-stationary data distributions,
due to the presence of latent and exogenous factors that influence the underlying data-generating process and separate market regimes.
When multiple agents are learning concurrently and updating a shared policy, the underlying environment as perceived by
any individual worker may appear to change over time.
This violation of the Markov property complicates the theoretical guarantees of convergence for standard RL algorithms and
necessitates novel approaches for robust policy learning.

Finally, the integration of asynchronous communication protocols, such as ZeroMQ, while beneficial for reducing messaging latency,
introduces its own set of challenges.
Ensuring that message passing remains robust under high load conditions and that the replay buffer is updated consistently are
critical for maintaining the overall stability of the system.
These issues become especially pronounced in high-frequency,
low-latency domains where even minor delays or inconsistencies can result in suboptimal performance.

In summary, although distributed RL systems offer significant potential by parallelizing both data collection and learning,
achieving efficient, stable, and scalable implementations remains a major research challenge.
Addressing communication overhead, data staleness, resource heterogeneity,
and non-stationarity is essential for deploying distributed RL in demanding applications such as financial trading and other real-time decision-making domains.
