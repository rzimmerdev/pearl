@Article{WOS:001311045100001,
    author         = {Li, Mingfei and Liu, Haibin and Xie, Feng and Huang, He},
    journal        = {ELECTRONICS},
    title          = {Adaptive Distributed Control for Leader-Follower Formation Based on a Recurrent SAC Algorithm},
    year           = {2024},
    month          = {SEP},
    number         = {17},
    volume         = {13},
    abstract       = {This study proposes a novel adaptive distributed recurrent SAC (Soft
   Actor-Critic) control method to address the leader-follower formation
   control problem of omnidirectional mobile robots. Our method
   successfully eliminates the reliance on the complete state of the leader
   and achieves the task of formation solely using the pose between robots.
   Moreover, we develop a novel recurrent SAC reinforcement learning
   framework that ensures that the controller exhibits good transient and
   steady-state characteristics to achieve outstanding control performance.
   We also present an episode-based memory replay buffer and sampling
   approaches, along with a unique normalized reward function, which
   expedites the recurrent SAC reinforcement learning formation framework
   to converge rapidly and receive consistent incentives across various
   leader-follower tasks. This facilitates better learning and adaptation
   to the formation task requirements in different scenarios. Furthermore,
   to bolster the generalization capability of our method, we normalized
   the state space, effectively eliminating differences between formation
   tasks of different shapes. Different shapes of leader-follower formation
   experiments in the Gazebo simulator achieve excellent results,
   validating the efficacy of our method. Comparative experiments with
   traditional PID and common network controllers demonstrate that our
   method achieves faster convergence and greater robustness. These
   simulation results provide strong support for our study and demonstrate
   the potential and reliability of our method in solving real-world
   problems.},
    article-number = {3513},
    doi            = {10.3390/electronics13173513},
    ranking        = {rank5},
    type           = {Article},
    unique-id      = {WOS:001311045100001},
}

@Article{WOS:001020359500002,
    author         = {Zheng, Weijian and Wang, Dali and Song, Fengguang},
    journal        = {ACM TRANSACTIONS ON PARALLEL COMPUTING},
    title          = {A Distributed-GPU Deep Reinforcement Learning System for Solving Large Graph Optimization Problems},
    year           = {2023},
    month          = {JUN},
    number         = {2},
    volume         = {10},
    abstract       = {Graph optimization problems (such as minimum vertex cover, maximum cut,
   traveling salesman problems) appear in many fields including social
   sciences, power systems, chemistry, and bioinformatics. Recently, deep
   reinforcement learning (DRL) has shown success in automatically learning
   good heuristics to solve graph optimization problems. However, the
   existing RL systems either do not support graph RL environments or do
   not support multiple or many GPUs in a distributed setting. This has
   compromised the ability of reinforcement learning in solving large-scale
   graph optimization problems due to lack of parallelization and high
   scalability. To address the challenges of parallelization and
   scalability, we develop RL4GO, a high-performance distributed-GPU DRL
   framework for solving graph optimization problems. RL4GO focuses on a
   class of computationally demanding RL problems, where both the RL
   environment and policy model are highly computation intensive.
   Traditional reinforcement learning systems often assume either the RL
   environment is of low time complexity or the policy model is small.
   In this work, we distribute large-scale graphs across distributed GPUs
   and use the spatial parallelism and data parallelism to achieve scalable
   performance. We compare and analyze the performance of the spatial
   parallelism and data parallelism and show their differences. To support
   graph neural network (GNN) layers that take as input data samples
   partitioned across distributed GPUs, we design parallel mathematical
   kernels to perform operations on distributed 3D sparse and 3D dense
   tensors. To handle costly RL environments, we design a parallel graph
   environment to scale up all RL-environment-related operations. By
   combining the scalable GNN layers with the scalable RL environment, we
   are able to develop high-performance RL4GO training and inference
   algorithms in parallel. Furthermore, we propose two optimization
   techniques-replay buffer on-the-fly graph generation and adaptive
   multiple-node selection-to minimize the spatial cost and accelerate
   reinforcement learning. This work also conducts in-depth analyses of
   parallel efficiency and memory cost and shows that the designed RL4GO
   algorithms are scalable on numerous distributed GPUs. Evaluations on
   large-scale graphs show that (1) RL4GO training and inference can
   achieve good parallel efficiency on 192 GPUs, (2) its training time can
   be 18 times faster than the state-of-the-art Gorila distributed RL
   framework {[}34], and (3) its inference performance achieves a 26 times
   improvement over Gorila.},
    article-number = {6},
    doi            = {10.1145/3589188},
    ranking        = {rank5},
    type           = {Article},
    unique-id      = {WOS:001020359500002},
}

@Article{WOS:001360093300001,
    author          = {Abbas, Muhammad Naveed and Liston, Paul and Lee, Brian and Qiao, Yuansong},
    journal         = {KNOWLEDGE-BASED SYSTEMS},
    title           = {CESDQL: Communicative experience-sharing deep Q-learning for scalability in multi-robot collaboration with sparse reward},
    year            = {2024},
    month           = {DEC 20},
    volume          = {306},
    abstract        = {Owing to the massive transformation in industrial processes and
   logistics, warehouses are also undergoing advanced automation. The
   application of Autonomous Mobile Robots (a.k.a. multi-robots) is one of
   the important elements of overall warehousing automation. The autonomous
   collaborative behaviour of the multi- robots can be considered as
   employment on a control task and, thus, can be optimised using
   multi-agent reinforcement learning (MARL). Consequently, an autonomous
   warehouse is to be represented by an MARL environment. An MARL
   environment replicating an autonomous warehouse poses the challenge of
   exploration due to sparse reward leading to inefficient collaboration.
   This challenge aggravates further with an increase in the number of
   robots and the grid size, i.e., scalability. This research proposes C
   ommunicative E xperience- S haring D eep Q-Learning (CESDQL) based on
   Q-learning, a novel hybrid multi-robot communicative framework for
   scalability for MARL collaboration with sparse rewards, where
   exploration is challenging and makes collaboration difficult. CESDQL
   makes use of experience-sharing through collective sampling from the
   Experience (Replay) buffer and communication through Communicative Deep
   recurrent Q-network (CommDRQN), a Qfunction approximator. Through
   empirical evaluation of CESDQL in a variety of collaborative scenarios,
   it is established that CESDQL outperforms the baselines in terms of
   convergence and stable learning. Overall, CESDQL achieves 5\%, 69\%,
   60\%, 211\%, 171\%, 3.8\% \& 10\% more final accumulative training
   returns than the closest performing baseline by scenario, and, 27\%,
   10.33\% \& 573\% more final average training returns than the closest
   performing baseline by the big-scale scenario.},
    article-number  = {112714},
    doi             = {10.1016/j.knosys.2024.112714},
    earlyaccessdate = {NOV 2024},
    ranking         = {rank5},
    type            = {Article},
    unique-id       = {WOS:001360093300001},
}

@InCollection{WOS:000895738900040,
    author    = {He, Hang and Ren, Tao and Cui, Meng and Liu, Dong and Niu, Jianwei},
    booktitle = {WIRELESS ALGORITHMS, SYSTEMS, AND APPLICATIONS, PT III},
    title     = {Deep Reinforcement Learning Based Computation Offloading in Heterogeneous MEC Assisted by Ground Vehicles and Unmanned Aerial Vehicles},
    year      = {2022},
    editor    = {Wang, L and Segal, M and Chen, J and Qiu, T},
    pages     = {481-494},
    series    = {Lecture Notes in Computer Science},
    type      = {Proceedings Paper},
    volume    = {13473},
    abstract  = {Compared with traditional mobile edge computing (MEC), heterogeneous MEC
   (H-MEC), which is assisted by ground vehicles (GVs) and unmanned aerial
   vehicles (UAVs) simultaneously, is attracting more and more attention
   from both academia and industry. By deploying base stations (along with
   edge servers) on GVs or UAVs, H-MEC is more suitable for access-demand
   dynamicallychanging network environments, e.g., sports matches, traffic
   management, and emergency rescue. However, it is non-trivial to perform
   real-time user association and resource allocation in large-scale H-MEC
   environments. Motivated by this, we propose a shared multi-agent
   proximal policy optimization (SMAPPO) algorithm based on the centralized
   training and distributed execution framework. Due to the NP-hard
   difficulty of jointly optimizing user association and resource
   allocation for H-MEC, we adopt the actor-critic-based online-policy
   gradient (PG) algorithm to obtain near-optimal solutions with low
   scheduling complexities. In addition, considering the low sampling
   efficiency of PG, we introduce proximal policy optimization to increase
   the training efficiency by importance sampling. Moreover, we leverage
   the idea of centralized training and distributed execution to improve
   the training efficiency and reduce scheduling complexity, so that each
   mobile device makes decisions based only on local observation and learns
   other MDs' experience from a shared replay buffer. Extensive simulation
   results demonstrate that SMAPPO can achieve more satisfactory
   performances than traditional algorithms.},
    doi       = {10.1007/978-3-031-19211-1\_40},
    ranking   = {rank5},
    unique-id = {WOS:000895738900040},
}

@InCollection{WOS:000719386001131,
    author            = {Lu, Haodong and Wang, Kun},
    booktitle         = {IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2021)},
    title             = {Distributed Machine Learning based Mitigating Straggler in Big Data Environment},
    year              = {2021},
    series            = {IEEE International Conference on Communications},
    type              = {Proceedings Paper},
    abstract          = {In big data era, utilizing the parameter server paradigm has been
   regarded as an efficient and practical way to improve performance in
   processing deep learning (DL) applications. One of the main problems is
   that straggler greatly hinders DL training progress, but the previous
   methods cannot fully consider the resource utilization of the cluster
   when dealing with straggler. To mitigate straggler problem in parameter
   server, we propose a Deep Reinforcement Learning (DRL)-based framework
   called Distributed Actor-critic Reinforcement Learning (DARL) that can
   automatically adapt each worker's training load to the dynamic cluster
   without parameter settings. DARL employs state-of-the-art techniques to
   stabilize training and improve convergence, including distributed
   framework, multiple actors and prioritized experience replay. Meanwhile,
   we also apply our customized experience sampling method to fully exploit
   potentially good samples. Experiments using real DL workloads show that
   DARL outperforms the representative Bulk Synchronous Parallel (BSP)
   scheme by 57.8\% and Stale Synchronous Parallel (SSP) by 503\% in terms
   of per-iteration time in heterogeneous environment.},
    book-group-author = {IEEE},
    doi               = {10.1109/ICC42927.2021.9500531},
    ranking           = {rank5},
    unique-id         = {WOS:000719386001131},
}

@Article{WOS:000953396500001,
    author    = {Song, Yujae and Lim, Sung Hoon and Jeon, Sang-Woon},
    journal   = {IEEE ACCESS},
    title     = {Handover Decision Making for Dense HetNets: A Reinforcement Learning Approach},
    year      = {2023},
    pages     = {24737-24751},
    volume    = {11},
    abstract  = {In this paper, we consider the problem of decision making in the context
   of a dense heterogeneous network with a macro base station and multiple
   small base stations. We propose a deep Q-learning based algorithm that
   efficiently minimizes the overall energy consumption by taking into
   account both the energy consumption from transmission and overheads, and
   various network information such as channel conditions and causal
   association information. The proposed algorithm is designed based on the
   centralized training with decentralized execution (CTDE) framework in
   which a centralized training agent manages the replay buffer for
   training its deep Q-network by gathering state, action, and reward
   information reported from the distributed agents that execute the
   actions. We perform several numerical evaluations and demonstrate that
   the proposed algorithm provides significant energy savings over other
   contemporary mechanisms depending on overhead costs, especially when
   additional energy consumption is required for handover procedure.},
    doi       = {10.1109/ACCESS.2023.3254557},
    ranking   = {rank5},
    type      = {Article},
    unique-id = {WOS:000953396500001},
}

@Article{WOS:001283953100001,
    author          = {Jia, Kunkun and Xia, Hui and Zhang, Rui and Sun, Yue and Wang, Kai},
    journal         = {COMPUTER NETWORKS},
    title           = {Multi-agent DRL for edge computing: A real-time proportional compute offloading},
    year            = {2024},
    month           = {OCT},
    volume          = {252},
    abstract        = {In the Industrial Internet of Things, devices with limited computing
   power and energy storage often rely on offloading tasks to edge servers
   for processing. However, existing methods are plagued by the high cost
   of device communication and unstable training processes. Consequently,
   Deep reinforcement learning (DRL) has emerged as a promising solution to
   tackle the computation offloading problem. In this paper, we propose a
   framework called multi-agent twin delayed shared deep deterministic
   policy gradient algorithm (MASTD3) based on DRL. Firstly, we formulate
   the task offloading conundrum as a long-term optimization problem, which
   aids in mitigating the challenge of deciding between local or remote
   task execution by a device, leading to more effective task offloading
   management. Secondly, we enhance MASTD3 by introducing a priority
   experience replay buffer mechanism and a model sample replay buffer
   mechanism, thus improving sample utilization and overcoming the
   cold-start problem associated with long-term optimization. Moreover, we
   refine the actor critic structure, enabling all agents to share the same
   critic network. This modification accelerates convergence speed during
   the training process and reduces computational costs during runtime.
   Finally, experimental results demonstrate that MASTD3 effectively
   addresses the proportional offloading problem, which is optimized by
   44.32\%, 29.26\%, and 17.47\% compared to DDPQN, MADDPG, and FLoadNet.},
    article-number  = {110665},
    doi             = {10.1016/j.comnet.2024.110665},
    earlyaccessdate = {JUL 2024},
    ranking         = {rank5},
    type            = {Article},
    unique-id       = {WOS:001283953100001},
}

@InCollection{WOS:000852715901065,
    author            = {Li, Kaiyuan and Wang, Pengfei and Li, Chenliang},
    booktitle         = {PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR `22)},
    title             = {Multi-Agent RL-based Information Selection Framework for Sequential Recommendation},
    year              = {2022},
    pages             = {1622-1631},
    type              = {Proceedings Paper},
    abstract          = {For sequential recommender, the coarse-grained yet sparse sequential
   signals mined from massive user-item interactions have become the
   bottleneck to further improve the recommendation performance. To
   alleviate the spareness problem, exploiting auxiliary semantic features
   (e.g., textual descriptions, visual images and knowledge graph) to
   enrich contextual information then turns into a mainstream methodology.
   Though effective, we argue that these different heterogeneous features
   certainly include much noise which may overwhelm the valuable sequential
   signals, and therefore easily reach the phenomenon of negative
   collaboration (i.e., 1 + 1 < 2). How to design a flexible strategy to
   select proper auxiliary information and alleviate the negative
   collaboration towards a better recommendation is still an interesting
   and open question. Unfortunately, few works have addressed this
   challenge in sequential recommendation.
   In this paper, we introduce a Multi-Agent RL-based Information Selection
   Model (named MARIS) to explore an effective collaboration between
   different kinds of auxiliary information and sequential signals in an
   automatic way. Specifically, MARIS formalizes the auxiliary feature
   selection as a cooperative Multi-agent Markov Decision Process. For each
   auxiliary feature type, MARIS resorts to using an agent to determine
   whether a specific kind of auxiliary feature should be imported to
   achieve a positive collaboration. In between, a QMIX network is utilized
   to cooperate their joint selection actions and produce an episode
   corresponding an effective combination of different auxiliary features
   for the whole historical sequence. Considering the lack of supervised
   selection signals, we further devise a novel reward-guided sampling
   strategy to leverage exploitation and exploration scheme for episode
   sampling. By preserving them in a replay buffer, MARIS learns the
   action-value function and the reward alternatively for optimization.
   Extensive experiments on four real-world datasets demonstrate that our
   model obtains significant performance improvement over up-to-date
   state-of-the-art recommendation models.},
    book-group-author = {ACM},
    doi               = {10.1145/3477495.3532022},
    ranking           = {rank5},
    unique-id         = {WOS:000852715901065},
}

@Article{WOS:001354561000001,
    author    = {Lu, Jixiang and Xie, Zhangtian and Xu, Hongsheng and Liu, Junjun},
    journal   = {IEEE ACCESS},
    title     = {Optimizing Joint Bidding and Incentivizing Strategy for Price-Maker Load Aggregators Based on Multi-Task Multi-Agent Deep Reinforcement Learning},
    year      = {2024},
    pages     = {163988-164001},
    volume    = {12},
    abstract  = {The increasing penetration of renewable energy sources poses significant
   challenges for modern power systems, particularly in supply-demand
   balance and peak regulation. Load aggregators (LAs) play a crucial role
   by integrating small to medium-sized loads and coordinating demand
   response (DR). However, previous research works ignored the inherent
   coupling between price-maker LAs' decision-making of bidding price and
   quantity in the ancillary service market and decision-making of
   incentive price in DR. This study introduces a joint bidding and
   incentivizing model for a price-maker LA participating in a
   peak-regulation ancillary service market (PRM) and developing an
   incentive-based demand response (IBDR), where the LA's objective is to
   maximize its long-term cumulative payoff. In order to solve this complex
   joint decision-making optimization problem more effectively and
   efficiently, a model-free multi-task multi-agent deep reinforcement
   learning-based (MTMA-DRL-based) method incorporating a shared,
   centralized prioritized experience replay buffer (PERB) is proposed.
   Case studies in real-world settings confirm that the proposed model
   effectively captures the interdependence between bidding price, bidding
   quantity, and incentive price decisions. The proposed MTMA-DRL-based
   method is also proven to outperform existing methods.},
    doi       = {10.1109/ACCESS.2024.3491189},
    ranking   = {rank5},
    type      = {Article},
    unique-id = {WOS:001354561000001},
}

@Article{WOS:001385578000004,
    author    = {Hou, Jing and Chen, Guang and Zhang, Ruiqi and Li, Zhijun and Gu, Shangding and Jiang, Changjun},
    journal   = {IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS},
    title     = {Spreeze: High-Throughput Parallel Reinforcement Learning Framework},
    year      = {2025},
    month     = {FEB},
    number    = {2},
    pages     = {282-292},
    volume    = {36},
    abstract  = {The promotion of large-scale applications of reinforcement learning (RL)
   requires efficient training computation. While existing parallel RL
   frameworks encompass a variety of RL algorithms and parallelization
   techniques, the excessively burdensome communication frameworks hinder
   the attainment of the hardware's limit for final throughput and training
   effects on a single desktop. In this article, we propose Spreeze, a
   lightweight parallel framework for RL that efficiently utilizes a single
   desktop hardware resource to approach the throughput limit. We
   asynchronously parallelize the experience sampling, network update,
   performance evaluation, and visualization operations, and employ
   multiple efficient data transmission techniques to transfer various
   types of data between processes. The framework can automatically adjust
   the parallelization hyperparameters based on the computing ability of
   the hardware device in order to perform efficient large-batch updates.
   Based on the characteristics of the ``Actor-Critic{''} RL algorithm, our
   framework uses dual GPUs to independently update the network of actors
   and critics in order to further improve throughput. Simulation results
   show that our framework can achieve up to 15,000 Hz experience sampling
   and 370,000 Hz network update frame rate using only a personal desktop
   computer, which is an order of magnitude higher than other mainstream
   parallel RL frameworks, resulting in a 73\% reduction of training time.
   Our work on fully utilizing the hardware resources of a single desktop
   computer is fundamental to enabling efficient large-scale distributed RL
   training.},
    doi       = {10.1109/TPDS.2024.3497986},
    ranking   = {rank5},
    type      = {Article},
    unique-id = {WOS:001385578000004},
}

@InCollection{WOS:000864709905062,
    author            = {Fayaz, Muhammad and Yi, Wenqiang and Liu, Yuanwei and Nallanathan, Arumugam},
    booktitle         = {IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS (ICC 2022)},
    title             = {Throughput Optimization for SGF-NOMA via Distributed DRL with Prioritized Experience Replay},
    year              = {2022},
    pages             = {5178-5183},
    series            = {IEEE International Conference on Communications},
    type              = {Proceedings Paper},
    abstract          = {In this paper, we propose a novel distributed resource allocation
   mechanism for semi-grant-free non-orthogonal multiple access (SGF-NOMA)
   transmission to maximize the network throughput, where multi-agent deep
   reinforcement learning with prioritized experience replay (PER) is
   employed. We design a centralized training framework and decentralized
   decision making to increase the flexibility of the proposed scheme. More
   specifically, each grant-free user as an ``agent{''} learns the dynamics
   of the environment and makes its decisions independently in a
   decentralized manner. No heavy information exchange is needed to find
   the optimal transmit power and sub-channel that maximize the throughput.
   Numerical results show that the proposed algorithm with PER enhances the
   learning efficiency compared to the algorithm with conventional replay
   buffer and outperforms the existing scheme with a 12\% throughput
   increase.},
    book-group-author = {IEEE},
    ranking           = {rank5},
    unique-id         = {WOS:000864709905062},
}

@Article{WOS:001359244600219,
    author    = {Zhu, Shengchao and Han, Guangjie and Lin, Chuan and Tao, Qiuzi},
    journal   = {IEEE TRANSACTIONS ON MOBILE COMPUTING},
    title     = {Underwater Target Tracking Based on Hierarchical Software-Defined Multi-AUV Reinforcement Learning: A Multi-AUV Advantage-Attention Actor-Critic Approach},
    year      = {2024},
    month     = {DEC},
    number    = {12},
    pages     = {13639-13653},
    volume    = {23},
    abstract  = {With the rapid development of underwater robots, underwater
   communication techniques, etc., the Autonomous Underwater Vehicle (AUV)
   cluster network has emerged as a candidate paradigm to perform
   underwater civil and military applications, e.g., underwater target
   tracking. In this paper, we focus on how to utilize networking and
   multi-agent artificial intelligence technique to improve underwater
   target tracking. In particular, to improve the flexibility and
   scalability of the AUV cluster network, we employ Software-Defined
   Networking (SDN) and Centralized Training with Decentralized Execution
   (CTDE)-based Multi-Agent Reinforcement Learning (MARL) technologies, to
   propose a Hierarchical Software-Defined Multiple AUVs Reinforcement
   Learning (HSD-MARL) framework. For the MARL mechanism in HSD-MARL, we
   propose an advantage-attention mechanism and present the architecture of
   Multi-AUV Advantage-Attention Actor-Critic (MA-A3C), to address slow
   convergence and poor scalability issues on the AUV cluster network of
   large-scale. Further, to improve the utilization rate of advantage
   samples especially when the MA-A3C is utilized to perform AUV cluster
   network-based underwater tracking, we propose an `advantage resampling'
   method based on experience replay buffer. Evaluation results showcase
   that our proposed approaches can perform exact underwater target
   tracking based on AUV cluster network systems and outperform some recent
   research products in terms of convergence speed, tracking accuracy, etc.},
    doi       = {10.1109/TMC.2024.3437376},
    ranking   = {rank5},
    type      = {Article},
    unique-id = {WOS:001359244600219},
}

@InCollection{WOS:001268569303101,
    author            = {Kopic, Amna and Perenda, Erma and Gacanin, Haris},
    booktitle         = {2024 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE, WCNC 2024},
    title             = {Unveiling the Effects of Experience Replay on Deep Reinforcement Learning-based Power Allocation in Wireless Networks},
    year              = {2024},
    series            = {IEEE Wireless Communications and Networking Conference},
    type              = {Proceedings Paper},
    abstract          = {Deep reinforcement learning has emerged as a powerful tool for dynamic
   power allocation, as it allows continuous learning and real-time
   responsiveness to environmental changes through its core element,
   experience replay. Experience replay involves two critical
   hyperparameters: the replay buffer and mini-batch sizes. While
   state-of-the-art solutions primarily concentrate on designing input
   features and reward-shaping methods, the impact of experience replay
   parameters on system performance has often been overlooked. This paper
   aims to address this gap by exploring the effects of experience replay
   parameters in the context of dynamic power allocation in multi-carrier
   wireless systems. To address the power allocation problem, we propose a
   multi-agent cooperative deep reinforcement learning framework. The
   results show that a minimum of 2000 experiences in the replay buffer is
   necessary for the proposed solution to outperform conventional
   approaches. Moreover, many obsolete experiences within a larger replay
   buffer slightly decrease system performance. Interestingly, the increase
   in batch size does not significantly affect the learning models'
   training time due to parallel execution, yet, it improves performance.},
    book-group-author = {IEEE},
    doi               = {10.1109/WCNC57260.2024.10571107},
    ranking           = {rank5},
    unique-id         = {WOS:001268569303101},
}

@Article{WOS:001305610300001,
    author         = {Zhang, Hao and Du, Yu and Zhao, Shixin and Yuan, Ying and Gao, Qiuqi},
    journal        = {ELECTRONICS},
    title          = {VN-MADDPG: A Variable-Noise-Based Multi-Agent Reinforcement Learning Algorithm for Autonomous Vehicles at Unsignalized Intersections},
    year           = {2024},
    month          = {AUG},
    number         = {16},
    volume         = {13},
    abstract       = {The decision-making performance of autonomous vehicles tends to be
   unstable at unsignalized intersections, making it difficult for them to
   make optimal decisions. We propose a decision-making model based on the
   Variable-Noise Multi-Agent Deep Deterministic Policy Gradient
   (VN-MADDPG) algorithm to address these issues. The variable-noise
   mechanism reduces noise dynamically, enabling the agent to utilize the
   learned policy more effectively to complete tasks. This significantly
   improves the stability of the decision-making model in making optimal
   decisions. The importance sampling module addresses the inconsistency
   between outdated experience in the replay buffer and current
   environmental features. This enhances the model's learning efficiency
   and improves the robustness of the decision-making model. Experimental
   results on the CARLA simulation platform show that the success rate of
   decision making at unsignalized intersections by autonomous vehicles has
   significantly increased, and the pass time has been reduced. The
   decision-making model based on the VN-MADDPG algorithm demonstrates
   stable and excellent decision-making performance.},
    article-number = {3180},
    doi            = {10.3390/electronics13163180},
    ranking        = {rank5},
    type           = {Article},
    unique-id      = {WOS:001305610300001},
}

@InCollection{WOS:000827652300011,
    author    = {Furukawa, Masaki and Matsutani, Hiroki},
    booktitle = {30TH EUROMICRO INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND NETWORK-BASED PROCESSING (PDP 2022)},
    title     = {Accelerating Distributed Deep Reinforcement Learning by In-Network Experience Sampling},
    year      = {2022},
    editor    = {Gonzalez-Escribano, A and Garcia, JD and Torquati, M and Skavhaug, A},
    pages     = {75-82},
    series    = {Euromicro Conference on Parallel, Distributed and Network-Based Processing},
    type      = {Proceedings Paper},
    abstract  = {A computing cluster that interconnects multiple compute nodes is used to
   accelerate distributed reinforcement learning based on DQN (Deep
   Q-Network). In distributed reinforcement learning, Actor nodes acquire
   experiences by interacting with a given environment and a Learner node
   optimizes their DQN model. Since data transfer between Actor and Learner
   nodes increases depending on the number of Actor nodes and their
   experience size, communication overhead between them is one of major
   performance bottlenecks. In this paper, their communication performance
   is optimized by using DPDK (Data Plane Development Kit). Specifically,
   DPDK-based low-latency experience replay memory server is deployed
   between Actor and Learner nodes interconnected with a 40GbE (40Gbit
   Ethernet) network. Evaluation results show that, as a network
   optimization technique, kernel bypassing by DPDK reduces network access
   latencies to a shared memory server by 32.7\% to 58.9\%. As another
   network optimization technique, an in-network experience replay memory
   server between Actor and Learner nodes reduces access latencies to the
   experience replay memory by 11.7\% to 28.1\% and communication latencies
   for prioritized experience sampling by 21.9\% to 29.1\%.},
    doi       = {10.1109/PDP55904.2022.00020},
    ranking   = {rank4},
    unique-id = {WOS:000827652300011},
}

@Article{WOS:000970111200001,
    author    = {Goudarzi, Mohammad and Palaniswami, Marimuthu and Buyya, Rajkumar},
    journal   = {IEEE TRANSACTIONS ON MOBILE COMPUTING},
    title     = {A Distributed Deep Reinforcement Learning Technique for Application Placement in Edge and Fog Computing Environments},
    year      = {2023},
    month     = {MAY 1},
    number    = {5},
    pages     = {2491-2505},
    volume    = {22},
    abstract  = {Fog/Edge computing is a novel computing paradigm supporting
   resource-constrained Internet of Things (IoT) devices by placement of
   their tasks on edge and/or cloud servers. Recently, several Deep
   Reinforcement Learning (DRL)-based placement techniques have been
   proposed in fog/edge computing environments, which are only suitable for
   centralized setups. The training of well-performed DRL agents requires
   manifold training data while obtaining training data is costly. Hence,
   these centralized DRL-based techniques lack generalizability and quick
   adaptability, thus failing to efficiently tackle application placement
   problems. Moreover, many IoT applications are modeled as Directed
   Acyclic Graphs (DAGs) with diverse topologies. Satisfying dependencies
   of DAG-based IoT applications incur additional constraints and increase
   the complexity of placement problem. To overcome these challenges, we
   propose an actor-critic-based distributed application placement
   technique, working based on the IMPortance weighted Actor-Learner
   Architectures (IMPALA). IMPALA is known for efficient distributed
   experience trajectory generation that significantly reduces exploration
   costs of agents. Besides, it uses an adaptive off-policy correction
   method for faster convergence to optimal solutions. Our technique uses
   recurrent layers to capture temporal behaviors of input data and a
   replay buffer to improve the sample efficiency. The performance results,
   obtained from simulation and testbed experiments, demonstrate that our
   technique significantly improves execution cost of IoT applications up
   to 30\% compared to its counterparts.},
    doi       = {10.1109/TMC.2021.3123165},
    ranking   = {rank4},
    type      = {Article},
    unique-id = {WOS:000970111200001},
}

@Article{WOS:000900980700001,
    author         = {Lin, Zeyang and Lai, Jun and Chen, Xiliang and Cao, Lei and Wang, Jun},
    journal        = {ENTROPY},
    title          = {Curriculum Reinforcement Learning Based on K-Fold Cross Validation},
    year           = {2022},
    month          = {DEC},
    number         = {12},
    volume         = {24},
    abstract       = {With the continuous development of deep reinforcement learning in
   intelligent control, combining automatic curriculum learning and deep
   reinforcement learning can improve the training performance and
   efficiency of algorithms from easy to difficult. Most existing automatic
   curriculum learning algorithms perform curriculum ranking through expert
   experience and a single network, which has the problems of difficult
   curriculum task ranking and slow convergence speed. In this paper, we
   propose a curriculum reinforcement learning method based on K-Fold Cross
   Validation that can estimate the relativity score of task curriculum
   difficulty. Drawing lessons from the human concept of curriculum
   learning from easy to difficult, this method divides automatic
   curriculum learning into a curriculum difficulty assessment stage and a
   curriculum sorting stage. Through parallel training of the teacher model
   and cross-evaluation of task sample difficulty, the method can better
   sequence curriculum learning tasks. Finally, simulation comparison
   experiments were carried out in two types of multi-agent experimental
   environments. The experimental results show that the automatic
   curriculum learning method based on K-Fold cross-validation can improve
   the training speed of the MADDPG algorithm, and at the same time has a
   certain generality for multi-agent deep reinforcement learning algorithm
   based on the replay buffer mechanism.},
    article-number = {1787},
    doi            = {10.3390/e24121787},
    ranking        = {rank4},
    type           = {Article},
    unique-id      = {WOS:000900980700001},
}

@Article{WOS:001174207000001,
    author          = {Fan, Zhen and Zhang, Wei and Liu, Wenxin},
    journal         = {IEEE SYSTEMS JOURNAL},
    title           = {Data-Efficient Deep Reinforcement Learning-Based Optimal Generation Control in DC Microgrids},
    year            = {2024},
    month           = {MAR},
    number          = {1},
    pages           = {426-437},
    volume          = {18},
    abstract        = {Because of their simplicity and great energy-utilizing efficiency, dc
   microgrids are gaining popularity as an attractive option for the
   optimal operation of numerous distributed energy resources. The optimal
   power flow issue's nonlinearity and nonconvexity make it difficult to
   apply and develop the conventional control approach directly. With the
   development of machine learning in recent years, deep reinforcement
   learning (DRL) has been developed for solving such complex optimal
   control problems. This article proposes a DRL-based TD3 optimal control
   scheme to achieve the optimal generation control for dc microgrids. The
   generation cost of distributed generators is minimized, and the
   significant boundaries, such as generation bounds and the bus voltage
   bounds, are both guaranteed. The proposed approach connects the optimal
   control and reinforcement learning frameworks with centralized training
   and distributed execution structure. Case studies showed that
   reinforcement learning algorithms might optimize nonlinear and nonconvex
   systems with fast dynamics by utilizing particular reward function
   designs, data sampling, and constraint management strategies. In
   addition, producing the experience replay buffer before training
   drastically lowers learning failure, enhancing the data efficiency of
   the DRL process.},
    doi             = {10.1109/JSYST.2024.3355328},
    earlyaccessdate = {FEB 2024},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:001174207000001},
}

@Article{WOS:001085511600001,
    author    = {Al-Saffar, Mohammed and Gul, Mustafa},
    journal   = {IEEE ACCESS},
    title     = {Data-Efficient MADDPG Based on Self-Attention for IoT Energy Management Systems},
    year      = {2023},
    pages     = {109379-109389},
    volume    = {11},
    abstract  = {In this study, the simulated real-world Demand Response (DR) potential
   is controlled and optimized when household load characteristics are
   analyzed based on historical data information. To determine the optimal
   DR potential in smart homes integrated with IoT energy management
   systems, a multi-agent reinforcement learning framework can be one of
   the best solutions to handle various household appliances' control
   activities associated with stochastic nature. However, the main problem
   with multi-agent systems is a nonstationary environment that is arisen
   by the agents. Consequently, this can cause more system uncertainties.
   Hence, it requires an excessive number of interactions with the
   environment for training which leads to a data inefficient reinforcement
   learning model. Thus, we propose a new approach using a Multi-Agent Deep
   Deterministic Policy Gradient based on Bi-directional Long Short Term
   Memory and Attention Mechanism (BiLSTMA-MADDPG) to extract more useful
   information. Therefore, we developed an improved MADDPG model that
   exploits the BiLSTM layer to store a history of experience in the
   MADDPG's replay buffer, and the Attention Mechanism to reduce the model
   dependency upon the number of samples since it can extract the most
   valuable data and ignore the less important ones. In this way,
   BiLSTMA-MADDPG can perform better than the conventional MADDPG even with
   the small sample environment to motivate the exploration of a more
   robust and data-efficient regime. Therefore, the attention mechanism
   enables MADDPG to be more effective and scalable in learning in complex
   real-world multi-agent environments. Simulation results are obtained for
   a household environment with three cooperated agents to control the
   following devices, washing machine, air conditioner, and electric
   vehicle. The model performance is validated, showing an improvement to
   the data efficiency and convergence speed, and a promise for a real-life
   application in terms of appliance energy consumption.},
    doi       = {10.1109/ACCESS.2023.3322193},
    ranking   = {rank4},
    type      = {Article},
    unique-id = {WOS:001085511600001},
}

@Article{WOS:001108812800001,
    author          = {Sun, Haonan and Liu, Nian and Tan, Lu and Du, Peng and Zhang, Bocheng},
    journal         = {IET RENEWABLE POWER GENERATION},
    title           = {Digital twin-based online resilience scheduling for microgrids: An approach combining imitative learning and deep reinforcement learning},
    year            = {2024},
    month           = {JAN},
    number          = {1},
    pages           = {1-13},
    volume          = {18},
    abstract        = {Strong uncertainty of renewables puts high demands on the fast response
   of flexibility resources and resilience-oriented optimal scheduling for
   microgrids (MGs). Digital twins (DT) technology based on data-driven
   methods is a potential solution to this problem. A DT-based online
   resilience scheduling framework for MGs is designed in this study. Based
   on the proposed framework, a hybrid sequential-parallel combination
   method of imitation learning (IL) and deep reinforcement learning (DRL)
   is proposed to develop the optimal scheduling strategy for MGs. First, a
   mixed integer second-order cone programming (MISOCP) model is adopted to
   behave as an expert to generate decision demonstrations corresponding to
   operation scenarios of MGs. Then, IL and deep deterministic policy
   gradient (DDPG) are combined by (1) sequential pretrain and finetune and
   (2) parallel experience storing and sampling two steps to learn the
   optimal policy for MGs. Expert demonstrations from the MISOCP model are
   utilized to pretrain a deep neural network model by IL and initialize
   the policy network of DDPG to avoid it learning from scratch. Moreover,
   an expert replay buffer is introduced specifically to avoid forgetting
   the well-behaved expert experience and to further accelerate the
   training process. A 6-bus MG test system case study demonstrates the
   effectiveness and scalability of the proposed approach.
   A hybrid sequential-parallel combination method of imitation learning
   (IL) and deep reinforcement learning is proposed to learn the optimal
   policy. IL and deep deterministic policy gradient are combined to
   enhance the training performance by two steps: (1) sequential pretrain
   and finetune and (2) parallel experience storing and sampling. image},
    doi             = {10.1049/rpg2.12887},
    earlyaccessdate = {NOV 2023},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:001108812800001},
}

@InCollection{WOS:001012988000052,
    author            = {Wu, Yanbin and Yin, Zhishuai and Yu, Jia and Zhang, Ming},
    booktitle         = {2022 IEEE 7TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION ENGINEERING, ICITE},
    title             = {Lane Change Decision-Making through Deep Reinforcement Learning with Driver's Inputs},
    year              = {2022},
    pages             = {314-319},
    type              = {Proceedings Paper},
    abstract          = {Drivers usually make better decisions than the driving system in complex
   driving situations at the current level of autonomous driving. Combined
   with the driver decision-making advantage, a new Deep Reinforcement
   Learning architecture named dc-DRL is proposed for the lane change
   decision-making tasks in this study. Through the input of external
   drivers' control actions in experience replay buffer and corresponding
   revision in actor and critic network, safe and efficient lane change
   maneuvers can be achieved. The state and action space are notably
   treated as continuous, and the Deep Deterministic Policy Gradient
   algorithm is applied to our study. Extensive simulations are conducted
   in a realistic driving simulator, CARLA, to test the safety and
   efficiency performance in single-agent and multi-agent environment, and
   the results show that the proposed method outperforms the benchmark
   method in driving safety and efficiency for lane-change maneuvers, with
   a 32\% reduction in collision rate and a 13\% reduction in traveling
   time to complete a designed unit section.},
    book-group-author = {IEEE},
    doi               = {10.1109/ICITE56321.2022.10101421},
    ranking           = {rank4},
    unique-id         = {WOS:001012988000052},
}

@InCollection{WOS:001094862603091,
    author            = {Lai, Junyu and Liu, Huashuo and Sun, Yusong and Tan, Huidong and Gan, Lianqiang and Chen, Zhiyong},
    booktitle         = {ICC 2023-IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS},
    title             = {Multi-agent Deep Reinforcement Learning Aided Computing Offloading in LEO Satellite Networks},
    year              = {2023},
    pages             = {3438-3443},
    series            = {IEEE International Conference on Communications},
    type              = {Proceedings Paper},
    abstract          = {Legacy computing offloading approaches are originally designed for the
   terrestrial networks with rather static topologies, and may not be
   appropriate for the next-generation LEO satellite broadband networks
   (LSBNs) featured with high dynamicity. This paper presents a multi-agent
   deep reinforcement learning (MADRL) algorithm for making edge computing
   multi-level offloading decisions in the LSBNs. Particularly, computing
   offloading is formulated as a partially observable Markov decision
   process (POMDP) based multi-agent decision problem. Each LEO satellite
   is an intelligent agent, either conducting a received edge computing
   task or forwarding it to its four neighboring satellite or the nearest
   cloud node on the ground. These agents are fully cooperative and their
   deep neural network models used to make offloading decisions share the
   same parameter values and are trained by the same replay buffer. A
   centralized training and distributed execution framework is utilized to
   ensure that globally optimized offloading decisions can be achieved
   based on local observations. Comparative simulation experiments for six
   representative offloading approaches show that the proposed MADRL aided
   approach outperforms the others regarding to decreasing edge computing
   task processing delay and increasing onboard compute resource
   utilization ratio. In addition, the convergence of this MADRL aided
   approach is also the best among the three DRL-based approaches.},
    book-group-author = {IEEE},
    doi               = {10.1109/ICC45041.2023.10279759},
    ranking           = {rank4},
    unique-id         = {WOS:001094862603091},
}

@Article{WOS:001240091800001,
    author          = {Fan, Chenchen and Xu, Hongyu and Wang, Qingling},
    journal         = {COMPUTER NETWORKS},
    title           = {Multi-agent deep reinforcement learning for trajectory planning in UAVs-assisted mobile edge computing with heterogeneous requirements},
    year            = {2024},
    month           = {JUN},
    volume          = {248},
    abstract        = {In heterogeneous wireless networks, massive user equipments (UEs)
   generate computing tasks with timevarying heterogeneous requirements. To
   improve the service quality, this paper formulates a unmanned aerial
   vehicles (UAVs)-assisted mobile edge computing (MEC) framework for time
   -varying heterogeneous task requirements. In the framework, the task
   delay and the number of successfully executed tasks are optimized by
   jointly controlling the trajectories of multiple UAVs. To address the
   considered trajectory planning optimization problem, a collaborative
   multi -agent deep reinforcement learning (MADRL) algorithm is proposed,
   where each UAV is regarded as a learning agent. First, a counterfactual
   inference based personalized policy update mechanism is proposed to
   evaluate the independent policy of agents by comparing the policy with a
   designed counterfactual policy. Based on this idea, each agent updates a
   personalized policy from both group and individual interests to improve
   its cooperation ability in dynamic and complex environments. Then, a
   diversified experience sampling mechanism is proposed to enhance the
   efficiency of policy evaluation and update with rich experiences
   provided by the environment interaction and the modified whale
   optimization algorithm. Finally, evaluation results demonstrate the
   superiority and effectiveness of the proposed MADRL algorithm.},
    article-number  = {110469},
    doi             = {10.1016/j.comnet.2024.110469},
    earlyaccessdate = {MAY 2024},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:001240091800001},
}

@InCollection{WOS:001258380600136,
    author            = {Doanis, Pavlos and Spyropoulos, Thrasyvoulos},
    booktitle         = {2024 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS, ICNC},
    title             = {Multi-agent DQN with sample-efficient updates for large inter-slice orchestration problems},
    year              = {2024},
    pages             = {772-777},
    series            = {International Conference on Computer Networking and Communications},
    type              = {Proceedings Paper},
    abstract          = {Data-driven network slicing has been recently explored as a major driver
   for beyond 5G networks. Nevertheless, we are still a long way before
   such solutions are practically applicable in real problems.
   Reinforcement learning based solutions, addressing the problem of
   dynamically placing virtual network function chains on top of a physical
   topology, have to deal with astronomically high action spaces
   (especially in in multi-VNF, multi-domain, and multi-slice setups).
   Moreover, their training is not particularly data-efficient, which can
   pose shortcomings, given the scarce(r) availability of cellular network
   related data. Multi-agent DQN can reduce the action space complexity by
   many orders of magnitude compared to standard DQN. Nevertheless, these
   algorithms are data-hungry and convergence can still be slow. To this
   end, in this work we introduce two additional mechanisms on top of
   (multi-agent) DQN to speed up training. These mechanisms intelligently
   decide how to store to, and how to pick from the experience replay
   buffer, in order to achieve more efficient parameter updates (faster
   learning). The convergence speed gains of the proposed scheme are
   validated using real traffic data.},
    book-group-author = {IEEE},
    doi               = {10.1109/CNC59896.2024.10555923},
    ranking           = {rank4},
    unique-id         = {WOS:001258380600136},
}

@InCollection{WOS:001207755100255,
    author    = {Cho, Joohyun and Liu, Mingxi and Zhou, Yi and Chen, Rong-Rong},
    booktitle = {FIFTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS \& COMPUTERS, IEEECONF},
    title     = {Multi-Agent Recurrent Deterministic Policy Gradient with Inter-Agent Communication},
    year      = {2023},
    editor    = {Matthews, MB},
    pages     = {1394-1398},
    series    = {Conference Record of the Asilomar Conference on Signals Systems and Computers},
    type      = {Proceedings Paper},
    abstract  = {In this paper, we introduce a novel approach to multi-agent coordination
   under partial state and observation, called Multi-Agent Recurrent
   Deterministic Policy Gradient with Differentiable Inter-Agent
   Communication (MARDPG-IAC). In such environments, it is difficult for
   agents to obtain information about the actions and observations of other
   agents, which can significantly impact their learning performance. To
   address this challenge, we propose a recurrent structure that
   accumulates partial observations to infer the hidden information and a
   communication mechanism that enables agents to exchange information to
   enhance their learning effectiveness. We employ an asynchronous update
   scheme to combine the MARDPG algorithm with the differentiable
   inter-agent communication algorithm, without requiring a replay buffer.
   Through a case study of building energy control in a power distribution
   network, we demonstrate that our proposed approach outperforms
   conventional Multi-Agent Deep Deterministic Policy Gradient (MADDPG)
   that relies on partial state only.},
    doi       = {10.1109/IEEECONF59524.2023.10477063},
    ranking   = {rank4},
    unique-id = {WOS:001207755100255},
}

@Article{WOS:000868340000007,
    author          = {Yao, Xuyi and Chen, Ningjiang and Yuan, Xuemei and Ou, Pingjie},
    journal         = {FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE},
    title           = {Performance optimization of serverless edge computing function offloading based on deep reinforcement learning},
    year            = {2023},
    month           = {FEB},
    pages           = {74-86},
    volume          = {139},
    abstract        = {It is difficult for resource-constrained edge servers to simultaneously
   meet the performance require-ments of all the latency-sensitive Internet
   of Things (IoT) applications in edge computing. Therefore, it is a
   significant challenge to efficiently generate a task offloading
   strategy. Recently, deep reinforcement learning (DRL)-based task
   offloading methods have been studied to ensure long-term performance
   optimization. However, there are challenges in existing DRL-based task
   offloading methods, such as insufficient sample diversity and high
   exploration cost. To optimize the performance of edge computing and
   facilitate the development and deployment of event-driven IoT
   applications, the serverless edge computing model has emerged. It
   combines serverless computing, also known as Function as a Service
   (FaaS), with edge computing and has been adopted in edge AI inference
   and prediction, stream processing, face recognition, etc. In this paper,
   an experience-sharing deep reinforcement learning -based distributed
   function offloading method called ES-DRL is proposed in the setting of a
   combined stateful and stateless execution model for serverless edge
   computing. ES-DRL adopts a distributed learning architecture, where each
   edge FaaS (EFaaS) obtains the current state of the local environment and
   inputs them to the local DRL agent, which outputs the function
   offloading strategy. Then, each EFaaS uploads the experience data
   interacting with the environment to a global shared replay buffer
   located in the cloud and randomly draws a batch of data from it to
   optimize the parameters of the local network. A population-guided policy
   search method is introduced to speed up the convergence of the DRL agent
   and avoid falling into the local optimum. The experimental results
   demonstrate that ES-DRL can reduce the average latency by up to
   approximately 17 percent compared to the existing DRL-based task
   offloading method.(c) 2022 Elsevier B.V. All rights reserved.},
    doi             = {10.1016/j.future.2022.09.009},
    earlyaccessdate = {OCT 2022},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:000868340000007},
}

@Article{WOS:001167550200001,
    author          = {Li, Xinhang and Yang, Yiying and Yuan, Zheng and Wang, Zhe and Wang, Qinwen and Xu, Chen and Li, Lei and He, Jianhua and Zhang, Lin},
    journal         = {IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS},
    title           = {Progression Cognition Reinforcement Learning With Prioritized Experience for Multi-Vehicle Pursuit},
    year            = {2024},
    month           = {AUG},
    number          = {8},
    pages           = {10035-10048},
    volume          = {25},
    abstract        = {Multi-vehicle pursuit (MVP) such as autonomous police vehicles pursuing
   suspects is important but very challenging due to its mission and
   safety-critical nature. While multi-agent reinforcement learning (MARL)
   algorithms have been proposed for MVP in structured grid-pattern roads,
   the existing algorithms use random training samples in centralized
   learning, which leads to homogeneous agents showing low collaboration
   performance. For the more challenging problem of pursuing multiple
   evaders, these algorithms typically select a fixed target evader for
   pursuers without considering dynamic traffic situation, which
   significantly reduces pursuing success rate. To address the above
   problems, this paper proposes a Progression Cognition Reinforcement
   Learning with Prioritized Experience for MVP (PEPCRL-MVP) in urban
   multi-intersection dynamic traffic scenes. PEPCRL-MVP uses a
   prioritization network to assess the transitions in the global
   experience replay buffer according to each MARL agent's parameters. With
   the personalized and prioritized experience set selected via the
   prioritization network, diversity is introduced to the MARL learning
   process, which can improve collaboration and task-related performance.
   Furthermore, PEPCRL-MVP employs an attention module to extract critical
   features from dynamic urban traffic environments. These features are
   used to develop a progression cognition method to adaptively group
   pursuing vehicles. Each group efficiently targets one evading vehicle.
   Extensive experiments conducted with a simulator over unstructured roads
   of an urban area show that PEPCRL-MVP is superior to other
   state-of-the-art methods. Specifically, PEPCRLMVP improves pursuing
   efficiency by 3.95\% over Twin Delayed Deep Deterministic policy
   gradient-Decentralized Multi-Agent Pursuit and its success rate is
   34.78\% higher than that of Multi-Agent Deep Deterministic Policy
   Gradient. Codes are opensourced.},
    doi             = {10.1109/TITS.2024.3354196},
    earlyaccessdate = {JAN 2024},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:001167550200001},
}

@Article{WOS:000908349600008,
    author          = {Xie, Shaorong and Zhang, Zhenyu and Yu, Hang and Luo, Xiangfeng},
    journal         = {INFORMATION SCIENCES},
    title           = {Recurrent prediction model for partially observable MDPs},
    year            = {2023},
    month           = {JAN},
    pages           = {125-141},
    volume          = {620},
    abstract        = {Partially observable Markov decision process (POMDP) is a key
   challenging problem in the application of reinforcement learning since
   it comprehensively describes real agent -environment interactions.
   Recent works mainly utilize conventional reward signals to train a
   representation that converts POMDPs to MDPs. However, rewards alone are
   not enough for a good representation without temporal information. In
   this paper, we first introduce a novel Recurrent Prediction Model to
   integrate temporal information into the representa-tion that solves
   POMDP problems by training three additional unsupervised prediction
   models, named transition model, reward recovery model, and observation
   recovery model. This paper secondly makes a modification of the data
   structure of vanilla replay buffer to reduce the memory usage and
   thirdly proposes an off-policy correction algorithm to decrease the
   policy lag in POMDPs. The experiments show that our model achieves
   better performance in partially observable environments on both
   stand-alone and distributed training systems. (c) 2022 Elsevier Inc. All
   rights reserved.},
    doi             = {10.1016/j.ins.2022.11.065},
    earlyaccessdate = {NOV 2022},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:000908349600008},
}

@Article{WOS:001358880900001,
    author          = {Cai, Yingkai and Low, Kay-Soon and Wang, Zhaokui},
    journal         = {ADVANCES IN SPACE RESEARCH},
    title           = {Reinforcement learning-based satellite formation attitude control under multi-constraint},
    year            = {2024},
    month           = {DEC 1},
    number          = {11},
    pages           = {5819-5836},
    volume          = {74},
    abstract        = {As the complexity of space missions increases, the constraints on
   satellite attitude control become more stringent, particularly for
   satellites working in orbit formation. This paper introduces a novel
   method, based on the categorization and modeling of different
   constraints, for attitude control of satellite formations under multiple
   constraints. The method employs the Phased Priority Reinforcement
   Learning (PPRL) approach, which utilizes Deep Deterministic Policy
   Gradient (DDPG) technology. Considering the complexity of constraints
   and the challenge posed by the high control dimensionality due to
   multi-satellite coordination, the method addresses these challenges
   through a two-step training strategy. The first step addresses the
   multi-constraint issue for individual satellites and increases the
   priority of single-satellite training experience data in the experience
   replay buffer of the second step to enhance data utilization efficiency.
   To address the issue of reward sparsity in complex high-dimensional
   constraint models, a detailed reward mechanism is proposed,
   incorporating both local and global constraints into the reward
   function, thereby achieving both efficient and effective attitude
   control. This approach not only meets dynamic, state, and performance
   constraints but also demonstrates adaptability and robustness through
   numerical simulations. Compared to traditional methods, this approach
   achieves significant improvements in control performance and constraint
   satisfaction, offering a novel solution pathway for high-dimensional
   control problems in multi-constraint satellite formations. (c) 2024
   COSPAR. Published by Elsevier B.V. All rights are reserved, including
   those for text and data mining, AI training, and similar technologies.},
    doi             = {10.1016/j.asr.2024.07.084},
    earlyaccessdate = {NOV 2024},
    ranking         = {rank4},
    type            = {Article},
    unique-id       = {WOS:001358880900001},
}

@Article{WOS:001405883900027,
    author    = {Borzilov, Anatolii and Skrynnik, Alexey and Panov, Aleksandr},
    journal   = {IEEE ACCESS},
    title     = {Rethinking Exploration and Experience Exploitation in Value-Based Multi-Agent Reinforcement Learning},
    year      = {2025},
    pages     = {13770-13781},
    volume    = {13},
    abstract  = {Cooperative Multi-Agent Reinforcement Learning (MARL) focuses on
   developing strategies to effectively train multiple agents to learn and
   adapt policies collaboratively. Despite being a relatively new area of
   research, most MARL methods are based on well-established approaches
   used in single-agent deep learning tasks due to their proven
   effectiveness. In this paper, we focus on the exploration problem
   inherent in many MARL algorithms. These algorithms often introduce new
   hyperparameters and incorporate auxiliary components, such as additional
   models, which complicate the adaptation process of the underlying RL
   algorithm to better fit multi-agent environments. We aim to optimize a
   deep MARL algorithm with minimal modifications to the well-known QMIX
   approach. Our investigation of the exploitation-exploration dilemma
   shows that the performance of state-of-the-art MARL algorithms can be
   matched by a simple modification of the is an element of -greedy policy.
   This modification depends on the ratio of available joint actions to the
   number of agents. We also improve the training aspect of the replay
   buffer to decorrelate experiences based on recurrent rollouts rather
   than episodes. The improved algorithm is not only easy to implement, but
   also aligns with state-of-the-art methods without adding significant
   complexity. Our approach outperforms existing algorithms in four of
   seven scenarios across three distinct environments while remaining
   competitive in the other three.},
    doi       = {10.1109/ACCESS.2025.3530974},
    ranking   = {rank4},
    type      = {Article},
    unique-id = {WOS:001405883900027},
}
