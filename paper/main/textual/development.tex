%% USPSC-Cap2-Desenvolvimento.tex 

% ---
% Este capítulo, utilizado por diferentes exemplos do abnTeX2, ilustra o uso de
% comandos do abnTeX2 e de LaTeX.
% ---
%
%Asynchronous Shared Environments with State Consistency
%
%Asynchronous: Environments and Learners are decoupled, allowing for independent operation,
%with message queues acting as intermediate buffers for data exchange and synchronization.
%
%Shared: Multiple workers interact with a single environment instance, reducing redundant computations and memory overhead.
%
%State Consistency: Workers can access states previously observed by other workers, ensuring a synchronized view of the environment
%if a learner was unable to process the latest state in time for synchronization.
%
%This approach aims to improve computational efficiency and learning performance while maintaining training stability
%by allowing all agents to interact synchronously on all states due to the proposed consistent environment state design.


\chapter{Development}
\label{ch:development}
%Este capítulo é parte principal do trabalho acadêmico e deve conter a exposição ordenada e detalhada do assunto.
%Divide-se em seções e subseções, em conformidade com a abordagem do tema e do método, abrangendo:
%revisão bibliográfica, materiais e métodos, técnicas utilizadas, resultados obtidos e discussão.

Distributed reinforcement learning (RL) has evolved considerably in response to the demands of complex tasks such as game playing and financial trading.
Early methods such as the Asynchronous Advantage Actor-Critic (A3C)
algorithm introduced the concept of multiple parallel workers collecting experiences independently,
which helped to reduce update variance and speed up training.
Building on these ideas, architectures like IMPALA have decoupled data collection from model training by deploying a
centralized learner that periodically synchronizes its policy with a large number of worker agents.
This design enables significant scalability across heterogeneous computing resources and facilitates high-throughput training.

A key challenge addressed by these systems is the communication overhead incurred when transmitting experiences and
updated policies between the learner and the workers.
For example, IMPALA aggregates trajectories from multiple workers into
batches for efficient gradient computation while managing the staleness of policy information.
More recent approaches, such as SEED RL, have further optimized communication by bundling inference with training updates.
This reduces the latency associated with model parameter transmission and minimizes network congestion, thereby improving overall training efficiency.
In parallel, methods like Ape-X have introduced distributed prioritized experience replay, where critical experiences are sampled with higher probability,
enhancing data efficiency in scenarios where high-frequency data generation is crucial.

In the context of financial applications, distributed RL has inspired specialized solutions that integrate realistic market simulators.
These simulators often feature a limit order book (LOB) mechanism where buy and sell orders are submitted, queued, and matched in real time.
In such systems, the distributed architecture must handle not only the high throughput of simulation data but also the
precise timing of order execution and asynchronous policy updates.
Recent studies have demonstrated that merging shared environment simulations with asynchronous updates can improve both
computational efficiency and learning performance in trading scenarios.

Despite these advances, challenges persist—such as managing synchronization delays, mitigating data staleness,
and coping with non-stationarity—especially in environments where rapid decision making is essential.
The trade-offs between synchronous and asynchronous communication, efficient use of replay buffers, and resource heterogeneity remain open research questions.
This thesis builds upon these existing approaches by proposing a novel shared parallel environment architecture,
designed specifically to address these challenges in high-frequency trading applications.
In the following sections, we present the design and implementation of our proposed architecture,
which we refer to as the Parallel Environments for Asynchronous Reinforcement Learning (PEARL).

We start with an overview of related works in distributed RL and financial trading, followed by a detailed description of the PEARL architecture.

\input{textual/development/literature}

\section{Environment and Agent Methodology}
\label{sec:methodology}

This thesis makes several key contributions to the field of distributed reinforcement learning by introducing a novel shared
parallel environment architecture that addresses common inefficiencies in traditional systems.
Our approach, called Parallel Environments for Asynchronous Reinforcement Learning (PEARL),
is designed to enhance computational efficiency and learning performance in high-frequency trading applications.

First, the proposed framework enables multiple RL agents (workers) to share a single environment instance rather than running isolated simulations.
This design minimizes redundant computations and significantly reduces the communication overhead typically
encountered when synchronizing separate environment instances.
Second, the system incorporates an asynchronous policy update mechanism.
In our approach, workers continuously send their experience trajectories to a centralized learner without waiting for all processes to complete,
thereby mitigating issues of data staleness and enhancing convergence stability in non-stationary environments

Third, we validate the proposed architecture within the context of financial applications,
such as high-frequency trading, demonstrating that the shared environment
model can achieve superior computational efficiency and learning performance compared to conventional actor-learner designs.

\input{textual/development/environment}
\input{textual/development/agent}

\section{Technical Details and Framework Description}
\label{sec:technical_details}

\input{textual/development/distributed}
\input{textual/development/communication}
\input{textual/development/results}
