\chapter{Development}
\label{ch:development}

Distributed reinforcement learning (RL) has evolved considerably in response to the demands of complex tasks such as game playing and financial trading.
Early methods such as the Asynchronous Advantage Actor-Critic (A3C)~\citep{Mnih2016}
algorithm introduced the concept of multiple parallel workers collecting experiences independently,
which helped to increase training speeds and reduce reward variance.
Building on these ideas, architectures like IMPALA and Gorila~\citep{Espeholt2018,Nair2015} have decoupled data collection from model training by using
a decentralized learner and worker structure, which periodically synchronizes the policy with the updated gradient
from the workers through gradient passing.
The decentralized learner design enables significant scalability across heterogeneous computing resources and facilitates high-throughput training.

As mentioned in \fullref{ch:introduction}, a key challenge present in these systems is the communication overhead incurred when transmitting experiences or
policy gradient updates between the learner and the workers.
The IMPALA algorithm aggregates trajectories from multiple workers into
batches for efficient gradient computation while managing the staleness of policy information.
More recent approaches, such as SEED RL\citep{Espeholt2020}, have further optimized communication by bundling inference with training updates.
This reduces the latency associated with model parameter transmission and minimizes network congestion, thereby improving overall training efficiency.
In parallel, methods like Ape-X\citep{Horgan2018} have introduced distributed prioritized experience replay, where critical experiences are sampled with higher probability,
enhancing data efficiency in scenarios where high-frequency data generation is crucial.

In the context of financial applications, RL environments often integrate realistic market simulators.
These simulators statistically represent stylized facts of limit order book (LOB), which are discussed
further in \fullref{subsec:environment}.
For such simulators, the distributed architecture must handle not only the high throughput of simulation data but also the
precise timing of order execution and asynchronous policy updates.
This thesis builds upon existing distributed algorithms through means of shared parallel environments,
designed specifically to address the nature of high-frequency trading applications.
In the following sections, we present the design and implementation of our proposed architecture,
which we refer to as Parallel Environments for Asynchronous Reinforcement Learning (PEARL).

\section{Environment and Agent Methodology}
\label{sec:methodology}

As mentioned in \fullref{ch:introduction}, the proposed framework enables multiple RL agents
to share a single environment instance rather than running isolated simulations and thus
minimize redundant computations and communication overhead from synchronizing workers interacting with independent environment instances.
The dynamics of the simulator and the implemented agent used for testing the framework are described in the following subsections.

\input{textual/development/environment}
\input{textual/development/agent}

\section{Technical Details and Framework Description}
\label{sec:technical_details}

\input{textual/development/distributed}
\input{textual/development/communication}
