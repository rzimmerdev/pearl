\chapter{Development}
\label{ch:development}
%Este capítulo é parte principal do trabalho acadêmico e deve conter a exposição ordenada e detalhada do assunto.
%Divide-se em seções e subseções, em conformidade com a abordagem do tema e do método, abrangendo:
%revisão bibliográfica, materiais e métodos, técnicas utilizadas, resultados obtidos e discussão.
Distributed reinforcement learning (RL) has evolved considerably in response to the demands of complex tasks such as game playing and financial trading.
Early methods such as the Asynchronous Advantage Actor-Critic (A3C)
algorithm introduced the concept of multiple parallel workers collecting experiences independently,
which helped to reduce update variance and speed up training.
Building on these ideas, architectures like IMPALA and Gorila have decoupled data collection from model training by using
a decentralized learner and worker structure, which periodically synchronizes the policy with the updated gradient
from the workers through gradient passing.
This design enables significant scalability across heterogeneous computing resources and facilitates high-throughput training.

As mentioned in \autoref{ch:introduction}, a key challenge present in these systems is the communication overhead incurred when transmitting experiences or
updated policies between the learner and the workers.
The IMPALA algorithm aggregates trajectories from multiple workers into
batches for efficient gradient computation while managing the staleness of policy information.
More recent approaches, such as SEED RL, have further optimized communication by bundling inference with training updates.
This reduces the latency associated with model parameter transmission and minimizes network congestion, thereby improving overall training efficiency.
In parallel, methods like Ape-X have introduced distributed prioritized experience replay, where critical experiences are sampled with higher probability,
enhancing data efficiency in scenarios where high-frequency data generation is crucial.

In the context of financial applications, RL environments often integrate realistic market simulators.
These simulators statistically represent stylized facts of limit order book (LOB), which are discussed
further in \autoref{subsec:environment}.
For such simulators, the distributed architecture must handle not only the high throughput of simulation data but also the
precise timing of order execution and asynchronous policy updates.
This thesis builds upon existing distributed algorithms through means of shared parallel environments,
designed specifically to address the nature of high-frequency trading applications.
In the following sections, we present the design and implementation of our proposed architecture,
which we refer to as Parallel Environments for Asynchronous Reinforcement Learning (PEARL).

\input{textual/development/literature}

\section{Environment and Agent Methodology}
\label{sec:methodology}

As mentioned in \autoref{ch:introduction}, the proposed PEARL framework enables multiple RL agents (workers)
to share a single environment instance rather than running isolated simulations and thus
minimize redundant computations and communication overhead typically
encountered when synchronizing separate environment instances.
The dynamics of the simulator and the implemented agent used for testing the framework are described in the following subsections.

\input{textual/development/environment}
\input{textual/development/agent}

\section{Technical Details and Framework Description}
\label{sec:technical_details}

\input{textual/development/distributed}
\input{textual/development/communication}
