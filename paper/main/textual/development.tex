\chapter{Development}
\label{ch:development}
%Este capítulo é parte principal do trabalho acadêmico e deve conter a exposição ordenada e detalhada do assunto.
%Divide-se em seções e subseções, em conformidade com a abordagem do tema e do método, abrangendo:
%revisão bibliográfica, materiais e métodos, técnicas utilizadas, resultados obtidos e discussão.
Distributed reinforcement learning (RL) has evolved considerably in response to the demands of complex tasks such as game playing and financial trading.
Early methods such as the Asynchronous Advantage Actor-Critic (A3C)
algorithm introduced the concept of multiple parallel workers collecting experiences independently,
which helped to reduce update variance and speed up training.
Building on these ideas, architectures like IMPALA and Gorila have decoupled data collection from model training by using
a decentralized learner and worker structure, which periodically synchronizes the policy with the updated gradient
from the workers through gradient passing.
This design enables significant scalability across heterogeneous computing resources and facilitates high-throughput training.

As mentioned in \autoref{ch:introduction}, a key challenge present in these systems is the communication overhead incurred when transmitting experiences or
updated policies between the learner and the workers.
The IMPALA algorithm aggregates trajectories from multiple workers into
batches for efficient gradient computation while managing the staleness of policy information.
More recent approaches, such as SEED RL, have further optimized communication by bundling inference with training updates.
This reduces the latency associated with model parameter transmission and minimizes network congestion, thereby improving overall training efficiency.
In parallel, methods like Ape-X have introduced distributed prioritized experience replay, where critical experiences are sampled with higher probability,
enhancing data efficiency in scenarios where high-frequency data generation is crucial.

In the context of financial applications, RL environments often integrate realistic market simulators.
These simulators statistically represent stylized facts of limit order book (LOB), which are discussed
further in \autoref{subsec:environment}.
For such simulators, the distributed architecture must handle not only the high throughput of simulation data but also the
precise timing of order execution and asynchronous policy updates.
This thesis builds upon existing distributed algorithms through means of shared parallel environments,
designed specifically to address the nature of high-frequency trading applications.
In the following sections, we present the design and implementation of our proposed architecture,
which we refer to as Parallel Environments for Asynchronous Reinforcement Learning (PEARL).

\input{textual/development/literature}

\section{Environment and Agent Methodology}
\label{sec:methodology}

This thesis makes several key contributions to the field of distributed reinforcement learning by introducing a novel shared
parallel environment architecture that addresses common inefficiencies in traditional systems.
Our approach, called Parallel Environments for Asynchronous Reinforcement Learning (PEARL),
is designed to enhance computational efficiency and learning performance in high-frequency trading applications.

First, the proposed framework enables multiple RL agents (workers) to share a single environment instance rather than running isolated simulations.
This design minimizes redundant computations and significantly reduces the communication overhead typically
encountered when synchronizing separate environment instances.
Second, the system incorporates an asynchronous policy update mechanism.
In our approach, workers continuously send their experience trajectories to a centralized learner without waiting for all processes to complete,
thereby mitigating issues of data staleness and enhancing convergence stability in non-stationary environments

Third, we validate the proposed architecture within the context of financial applications,
such as high-frequency trading, demonstrating that the shared environment
model can achieve superior computational efficiency and learning performance compared to conventional actor-learner designs.

\input{textual/development/environment}
\input{textual/development/agent}

\section{Technical Details and Framework Description}
\label{sec:technical_details}

\input{textual/development/distributed}
\input{textual/development/communication}
\input{textual/development/results}
