%! Author = rzimmerdev
%! Date = 3/28/25

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amssymb}

% Document
\begin{document}
    \subsection{Simulated Trading Environment and Limit Order Book Dynamics}
    \label{subsec:environment}
    The simulated environment is designed to replicate key aspects of a modern market making dynamics by 
    incorporating a limit order book (LOB) mechanism.
    A LOB is the core structure in many electronic trading systems, where buy and sell orders are organized by price and time priority,
    and can be quoted continuously, thereby reflecting the real-time supply and demand of an asset.
    At any given moment, the orders available in the book are accessible to traders in a sorted fashion, organized per price level,
    as shown in~\cref{fig:lob}.

    Market participants interact with the exchange by submitting orders that represent their intention to buy, called a bid order,
    or their intention to sell, called an ask order.
    Specifically, each action corresponds to placing a limit order --- an order detailing a specified quantity at a predetermined price.
    When an agent submits an order, it is added to the appropriate side of the LOB, 
    and queued first according to its price and second to the time at which the order was submitted.
    An order is executed if there is a matching order on the opposite side of the book that satisfies the price condition,
    that is, if the price is higher or equal for a posted ask order or lower or equal for a posted bid order.
    Otherwise, if no existing order matches the posted order, 
    it remains in the book until a suitable counter-order appears or if it is cancelled by the trader.
    Similar to a limit order, another type of order is a market order, 
    which specifies no price and simply accepts to buy or sell the asset at the best available price; 

    The simulation employs a discrete-event framework to model these processes accurately.
    At each simulation step, the environment generates an observation that includes critical details on what changed in the LOB, 
    such as the best bid and ask prices, the order book depth across various price levels, and records of recent trade executions.
    These observations are then transformed into the state space and provided to the agents, 
    enabling their policies to act according to evolving market states.
    This event-driven process is incorporated into the simulator, 
    as it mimics the intricate dynamics of order matching exchanges observed in real markets.

    In our simulated environment, the processes of order submission and execution are handled asynchronously.
    Agents send their orders via a messaging layer implemented with the ZeroMQ framework, a brokerless messaging library,
    to ensure that multiple agents can interact with the shared LOB concurrently without incurring in significant latency.
    This asynchronous architecture not only improves simulation throughput but also closely aligns with
    the latency-sensitive nature of high-frequency trading environments, usually measured in the milliseconds.

    By integrating a realistic LOB into the simulation, the environment provides a robust benchmark for testing
    distributed reinforcement learning algorithms under complex stochastic market dynamics.
    The detailed representation of market microstructure, combined with fine-grained arrival of order events,
    allows for statistically representative simulations that ensure order submission strategies
    are adequately trained to adapt to market reaction.

    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.8\textwidth]{images/lob}
        \caption{Visualization of a limit order book (LOB) with buy and sell orders organized by price and time priority.}
        \label{fig:lob}
    \end{figure}

    Our LOB is implemented on top of a Red-Black tree data structure, which allows for efficient insertion, deletion,
    and sorted or reversed traversal per price level.
    Each price level is represented as a node in the tree, with the price as the key and a queue of posted order details
    (e.g., client id, quantity, timestamp) as the value.
    New orders are sampled from a mixture of processes, and at each time step, 
    the environment checks for crossing orders and executes trades accordingly,
    updating the order book and the agent's state, as well outputing the reward signal.

    The system dynamics are modeled as a continuous-time Markov Decision Process (MDP),
    where each state corresponds to some representation of the current market state.
    Our chosen state space contains both observed variables from the book, that is, $N$-depth price and quantity quotes,
    as well as fabricated features, such as $15$ window price moving averages,
    periodic volatility values, mean return rates, and others which we describe at the end of this section.
    The governing equation for the state dynamics is the Kolmogorov forward equation,
    which gives the probability of transitioning from some state $s \in S'$ 
    (where $S'$ is the set of all possible states excluding final states)
    to some other state $s' \in S$ (where $S$ is the set of all possible states)
    at a future time $t \geq 0$ given the agent's action in the state $s$ was $a$:
    \begin{equation}
        \frac{\partial P(s', t|s, a)}{\partial t} = \int_S L(x|s, a, t) P(s'|x, a, t) \, dx
        \label{eq:kolmogorov}
    \end{equation}

    Here, the action $a$ chosen by the control agent was sampled or deterministically chosen according to a policy $\pi(s)$,
    so that $a \in \pi(s)$ for deterministic policies or $a \sim \pi(s)$ for stochastic policies.
    Finally, \( L(x|s, a, t) \) is the generator operator for the environment's dynamics at any given time \( t \),
    and depends on the current state $s$ and the action taken by the agent $a$, and $P$ is the standard probability space.

    We are mostly interested in finding a policy that maximizes the expected reward over each episode (in our case, trading days).
    Solving for the generator operator $L$ results in finding the optimal policy $\pi*$, as it allows us to calculate
    the action with highest probability of resulting in the highest reward, but under complex market dynamics, 
    finding a closed form for $L$ can become extremely complex, 
    and oftentimes completel unfeasible due to the non-linear and non-stationary nature of asset-return distributions.
    Thus, solving for the transition probabilities is often done by either simplifying the market model and
    analytically finding closed-form solutions, as in earlier works~\citep{Avellaneda2008, Gueant2017},
    by numerically approximating the probabilities for state transitions for smaller state spaces,
    or by directly finding the distribution for $\pi*$. 
    In the latter case, we and solving the adjacent problem of finding a 
    policy to maximize the expected reward $E[R_t]$ for each state and action pairs
    (e.g., by simulating trajectories or using reinforcement learning methods)~\citep{Gueant2022, Gasperov2022, Guo2023}.
    This formulation makes up the reinforcement learning paradigm, and will be discussed in depth in~\fullref{subsec:agent}.
    
    The market-making problem can be modeled as a MDP,
    where the environment is represented as a simulated limit order book (LOB) that reflects stylized market behaviors,
    and the reward function as some score calculated in terms of the portfolio's daily or running Profit-and-Loss score.
    For our environment, we break up the simulating of order arrivals into four separate steps:
    \begin{itemize}
        \item Sampling the next event times;
        \item Sampling the spread, volatility and mean price drift processes;
        \item Sampling the number of new events, and their order prices and quantities;
        \item Inserting the sampled orders into the LOB.
    \end{itemize}
    This setup was designed to provide a highly realistic representation of known limit-order book and
    microstructure stylized facts, such as clustered order arrivals, normal return distributions and price drifts,
    which in turn guided our choice for adopting the Reinforcement Learning approach to optimize our policy instead of trying
    to find a closed-form solution.

    The event times in our simulator follow a Hawkes process,
    a stochastic process with a self-exciting kernel.
    This property is extremely relevant for the context of limit order books,
    as it can replicate the self-exciting nature of clustered order arrivals.
    The intensity kernel \( \lambda(t) \) for the Hawkes is defined as:

    \begin{equation}
        \begin{aligned}
            \lambda(t) = \mu + \sum_{t_i < t} \phi(t - t_i)\\
            \phi(t - t_i) = \alpha e^{-\beta(t - t_i)}
        \end{aligned}
        \label{eq:hawkes}
    \end{equation}

    where \( \mu > 0 \) is the baseline intensity, and \( \alpha \), \( \beta \) govern the magnitude and decay of past events' influence.
    To generate order arrivals, we can sample from an exponential process with intensity \( \lambda(t) \),
    making the inter-arrival times exponentially distributed.
    For the second step, the bid and ask prices are modeled as separate Geometric Brownian Motion (GBM) processes, 
    with prices evolving according to:
    \begin{gather}
        dX_{\text{ask}} = (\mu_t + s_t) X_{\text{mid}} \, dt + \sigma_t X_{\text{mid}} dW_t\\
        dX_{\text{bid}} = (\mu_t - s_t) X_{\text{mid}} \, dt + \sigma_t X_{\text{mid}} dW_t\\
        X_{\text{mid}} = \frac{X_{\text{ask}} + X_{\text{bid}}}{2}
        \label{eq:gbm}
    \end{gather}
    where \( \mu_t \) is the drift, \( s_t \) is the spread, and \(  \sigma_t \) is the volatility.
    To ensure our simulator is capable of replicating changing market dynamics,
    we model the GBM drift $\mu_t$, the spread $s_t$ and the volatility $\sigma$ separately.
    The drift \( \mu_t \) follows a mean-reverting Ornstein-Uhlenbeck process~\citep{Uhlenbeck1930},
    while the spread \( s_t \) is sampled from a Cox-Ingersoll-Ross process~\citep{Cox1985}, so we ensure it remains positive.
    Our price volatility is modeled using a GARCH(1,1) process:

    \begin{equation}
        \begin{aligned} 
            \sigma_t^2 &= \omega + \alpha \epsilon_{t-1}^2 + \beta \sigma_{t-1}^2
        \end{aligned}
    \end{equation}

    where \( \sigma_t \) is the volatility, \( \epsilon_{t-1} \) is the return shock, 
    and \( \alpha \), \( \beta \) are parameters capturing volatility clustering.
    The values for $\omega$, $\alpha$, and $\beta$, as well as the default simulation hyperparameters are
    available in the annexed code and reproducibility section (\Fullref{ch:code}).
    Finally, the midprice \( X_{\text{mid}} \) used in the GBM processes is calculated as the average of the current best bid and ask prices,
    as shown in \cref{eq:gbm}.
    In the absence of orders, the midprice is based on the last traded price or an initial value.
    Together with the sampled ask and bid prices, order quantities \( q \) are sampled from a Poisson distribution with a fixed rate,
    and the tuples of price, side (buy or sell), and quantity representing new orders and inserted into the limit order book,
    which automatically matches received orders.
    All market variables (e.g., spread, order arrival rate, and price drift) are sampled according to the four steps described above,
    and together with the Red-Black tree structure make up the underlying order book dynamics.

    After inserting the orders into the LOB, 
    the state observation for the RL environment is generated as a combination of $N$-depth levels of the limit order book 
    and additional observed and preprocessed features.
    For any given simulation time $t$, these features include the current midprice $X_\text{mid}$, the agent's inventory $\text{Inventory}_t$, 
    $10$, $15$ and $50$ window moving averages for the price returns, a $30$ period historical volatility $\hat{\sigma_t}$,
    a $15$ period RSI and order imbalance scores, calculated as:

        \begin{align}
        \label{eq:rsi}
            RSI_t = 1 - \frac{1}{1 + \frac{n_\text{up}}{n_\text{down}}}\\
            OI_t = \frac{Q_t^{\text{bid}} - Q_t^{\text{ask}}}{Q_t^{\text{bid}} + Q_t^{\text{ask}}}
        \end{align}
    
    These features together with the $N$-depth levels of the LOB, 
    represented by set of pairs of order prices and quantities $(\delta_t^{i}, Q_t^{i})_{i=1}^{N}$, 
    for both bid and ask sides,
    form the state space $s\in \mathcal{S}$:

    \begin{equation}
        \begin{aligned}
            s_{t} \in \mathcal{S} = \big\{ &\text{RSI}_t, \text{OI}_t, MP_{t}, \text{Inventory}_t, \hat{\sigma_t}\\
            & \text{MA}_{10, t}, \text{MA}_{15, t}, \text{MA}_{50, t}, \\
            & (\delta_t^{i, ask}, Q_t^{i, ask})_{i=1}^{N}, \\
            & (\delta_t^{i, bid}, Q_t^{i, bid})_{i=1}^{N} \big\}
        \end{aligned}\label{eq:equation}
    \end{equation}

    And the environment's action space is formed by the agent's pairs of spread and quantity continuous quotes
    $a_t \in \mathcal{A} = \left\{ \delta_t^\text{ask}, Q_t^\text{ask},  \delta_t^\text{bid}, Q_t^\text{bid} \right\}$.

\end{document}
