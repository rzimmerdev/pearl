%! Author = rzimmerdev
%! Date = 3/28/25

% Packages
% Document
\begin{document}
    \subsection{Parallel Environments for Asynchronous Reinforcement Learning (PEARL)}
    \label{subsec:pearl}
    The proposed architecture builds on the core idea of sharing a single, highly parallel environment among multiple agents while
    still allowing multiple environments to be run in parallel.
    In contrast to conventional distributed MARL systems—where each worker simulates an independent environment instance—
    the proposed shared environment unifies multiple independent simulations concurrently accessed by multiple workers
    (see~\fullimg{fig:pearl_architecture}).
    This design minimizes redundant computations and memory overhead,
    which is especially critical in high-frequency trading simulations where the fidelity and speed of the limit order book (LOB) dynamics are paramount.

    At the heart of the architecture lies a central service mesh that manages discovery of the available environments.
    Upon initialization, a learner can spawn as many workers processes as necessary,
    which in turn connect to discovered environments using a dealer/router pattern.
    In this setup, workers act as both data collectors and order executors:
    they send action vectors containing continuous order quotes to the LOB simulation,
    receive market updates, and record episodic trajectories that capture the sequence of states, actions, and rewards into a replay buffer.
    By sharing the environment, workers from different learners benefit from a consistent and synchronized view of the simulated market,
    which besides reducing the number of CPU time used to generate similar data to send to individual agents,
    also serves to enhance the quality of the observable experience data.

    Communication between the learner and workers is designed to be asynchronous.
    Workers gather their generated trajectories to the learner via the ZeroMQ asynchronous messaging layer,
    enabling workers to operate independently of each other—without waiting for unrelated environment steps—
    thereby reducing idle time and mitigating the straggler issues often encountered in distributed setups.
    The intrinsic details of the messaging pipeline is discussed further in~\fullref{subsec:communication}.

    % graphics for diagrams/pearl_architecture.pdf
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.8\textwidth]{diagrams/pearl.drawio.pdf}
        \caption{Overview of the Parallel Environments for Asynchronous Reinforcement Learning (PEARL) framework.}
        \label{fig:pearl_architecture}
    \end{figure}

    Meanwhile, the learner continuously processes incoming trajectories from the workers,
    and updates the global policy using the RL algorithm after the environments are all finished.
    This cycle ensures that while each worker may temporarily operate on slightly outdated policy parameters,
    the overall system converges steadily as fresh data are incorporated into the learner's updates.

    By allowing the workers to share a common environment and asynchronously gather trajectories,
    the proposed architecture aims to strike a balance between computational efficiency and scalability.
    The worker pool design also contributes to reducing the overall system latency,
    as workers can operate and unlock individual cores and sockets while waiting for the environment to synchronize other agents and return a new state.
    In traditional architectures, the cost of transferring model parameters and simulation data between isolated environment instances can introduce significant delays.
    By contrast, our approach tries to minimize such overhead and reduce communication delays for HFT applications by centralizing the learner weights.

    \begin{algorithm}
        \begin{algorithmic}[1]
            \Require Environments, Policy $\pi_{\theta}$, Rollout length $T$
            \State Initialize empty trajectory buffers for each environment

            \For{each environment $i$ in Environments}
                \State Reset environment and observe initial state $\mathbf{s}_i$
            \EndFor
            \For {each rollout step $t$ until rollout length $T$}
                \State Select actions $\mathbf{a} \sim \pi_{\theta}(\mathbf{s})$
                \State Send actions $\mathbf{a}$ to each corresponding environment asynchronously
                \State \textbf{async for} {each environment $i$ in Environments}
                \State $\quad$ Receive response $\mathbf{r}_i$ from environment $i$
                \State $\quad$ Store transition $(\mathbf{s}_i, \mathbf{a}_i, \mathbf{r}_i)$ in the trajectory buffer
                \State $\quad$ Set $\mathbf{s}_i \leftarrow \mathbf{s'}_i$
                \State $\quad$ \textbf{if} environment $i$ is done
                \State $\quad$$\quad$Remove environment $i$ from current rollout environments
                \State $\quad$ \textbf{end if}
                \State \textbf{end for}
            \EndFor
            \State $\textbf{Return}$ trajectory buffers
        \end{algorithmic}
        \caption{Asynchronous Trajectory Gathering}
        \label{alg:trajectory}
    \end{algorithm}

    Internally, the learner creates a worker pool before starting the training loop, and is then responsible
    for sending each step request asynchronously to each environment's router.
    The environment then waits for either all actions to be received or for a constant timeout interval to be reached, returning the results to the learner.
    Since all environments are executed in separate processes/distributed cores, the learner can continue to process the
    received trajectories while waiting for each asynchronous environment to respond,
    and thus the rollout speed is determined by the speed of the slowest environment.
    This becomes a significant bottleneck if the environments have different processing speeds,
    but due to the nature of the limit order book simulation, the processing time is, although large,
    still relatively consistent across all environments, and the fixed timeout rate also contributes to minimizing stragglers.
    A diagram of the trajectory the individual observations make before reaching the agents' replay buffers within the PEARL framework
    is shown in \fullimg{fig:pipeline}.

    \begin{figure}
        \centering
        \includegraphics[width=1\textwidth]{diagrams/pipeline.drawio.pdf}
        \caption{Illustration of the pipeline between learners, their workers and a single environment instance.}
        \label{fig:pipeline}
    \end{figure}

    In summary, this integrated system architecture—comprising a shared simulation environment, an asynchronous actor–learner framework, and
    a robust communication backbone—addresses many of the limitations inherent in earlier distributed reinforcement learning approaches.
    By combining the advantages of shared resource utilization with scalable, asynchronous updates, the architecture provides a solid foundation for
    developing reinforcement learning systems capable of operating effectively in complex, dynamic domains.
    We implemented both the independent process workers and shared process for learner/workers,
    and decided to use the shared process due to the increased efficiency of running the forward and backward batched passes
    for all environments being faster than the speed gain of running the rollout in parallel.
    That is, instead of sampling actions for each observation independently per worker,
    we gather the state buffers into the centralized learner and perform a batched forward pass on all state tensors.

    We compared the policy act times for both approaches in~\fullref{sec:results},
    and as expected, this did not affect the environment step times in any way.
    It did affect the policy speed, as sampling policy actions on individual environment states instead of
    centralizing the state variable and using a batched approach resulted in significantly longer update times\footnote{
        We attributed the difference in processing times to the overhead of sending and retrieving data from the GPU.
        Even though performing this step in a distributed fashion could increase processing speeds if more GPU computing power was used,
        discussions regarding distributed learners have already been extensively explored in works such as IMPALA and APE-X,
        and are beyond the scope of this thesis.
    }.
    This comparison was our main motivation behind centralizing the actor act method into a batched space tensor
    before performing the policy network forward pass.

\end{document}
