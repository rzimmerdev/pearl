%! Author = rzimmerdev
%! Date = 3/28/25

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{algpseudocode}

% Document
\begin{document}
    \subsection{Parallel Environments for Asynchronous Reinforcement Learning (PEARL)}
    \label{subsec:pearl}
    The proposed architecture builds on the core idea of sharing a single, highly parallel environment among multiple agents while
    maintaining an efficient and scalable actor–learner framework.
    In contrast to conventional distributed MARL systems—where each worker simulates an independent environment instance—
    the proposed shared environment unifies multiple independent simulations concurrently accessed by multiple workers
    (see Figure~\ref{fig:pearl_architecture}).
    This design minimizes redundant computations and memory overhead,
    which is especially critical in high-frequency trading simulations where the fidelity and speed of the limit order book (LOB) dynamics are paramount.

    At the heart of the architecture lies a central learner that manages the global policy.
    Upon initialization, the learner spawns several worker processes using Python's multiprocessing library.
    Each worker retrieves the latest version of the shared policy and uses it to interact with the common environment.
    In this setup, workers act as both data collectors and order executors: they send orders (actions) to the LOB simulation,
    receive market updates, and construct trajectories that capture the sequence of states, actions, and rewards.
    By sharing the environment, workers benefit from a consistent and synchronized view of the simulated market,
    which enhances the quality of the experience data collected.

    Communication between the learner and workers is designed to be asynchronous.
    Workers submit their generated trajectories to the learner via a high-performance messaging layer implemented with ZeroMQ\@.
    This asynchronous mechanism enables workers to operate independently—without waiting for policy updates—
    thereby reducing idle time and mitigating the staleness issues often encountered in distributed setups

    % graphics for diagrams/pearl_architecture.pdf
    \begin{figure}[htb]
        \centering
        \includegraphics[width=0.8\textwidth]{diagrams/pearl_architecture.pdf}
        \caption{Overview of the Parallel Environments for Asynchronous Reinforcement Learning (PEARL) framework.}
        \label{fig:pearl_architecture}
    \end{figure}

    Meanwhile, the learner continuously processes incoming trajectories from the workers, updates the global policy using reinforcement learning algorithms,
    and the workers can call the shared agent parameters at any time.
    This cycle ensures that while each worker may temporarily operate on slightly outdated policy parameters,
    the overall system converges steadily as fresh data are incorporated into the learner's updates.

    By allowing the workers to share a common environment and asynchronously update the global policy,
    the proposed architecture aims to strike a balance between computational efficiency and learning performance.
    The computational load—especially during intensive simulations and gradient computations—is balanced effectively
    across the workers, due to the pooled resource utilization.
    The worker pool design also contributes to reducing the overall system latency,
    as workers can operate and unlock their individual threads while waiting for the environment to synchronize other agents and return a new state.
    In traditional architectures, the cost of transferring model parameters and simulation data between isolated environment instances can introduce significant delays.
    By contrast, our approach minimizes such overhead by centralizing the learner weights and leveraging efficient, low-latency communication channels.
    This architecture is particularly well-suited for real-time applications, such as high-frequency trading,
    where even minor delays can adversely affect performance.

    \begin{algorithm}
        \begin{algorithmic}[1]
            \Require Environments, Policy $\pi_{\theta}$, Rollout length $T$
            \State Initialize empty trajectory buffers for each environment

            \For{each environment $i$ in Environments}
                \State Reset environment and observe initial state $\mathbf{s}_i$
            \EndFor
            \For {each rollout step $t$ until rollout length $T$}
                \State Select actions $\mathbf{a} \sim \pi_{\theta}(\mathbf{s})$
                \State Send actions $\mathbf{a}$ to each corresponding environment asynchronously
                \State \textbf{async for} {each environment $i$ in Environments}
                \State $\quad$ Receive response $\mathbf{r}_i$ from environment $i$
                \State $\quad$ Store transition $(\mathbf{s}_i, \mathbf{a}_i, \mathbf{r}_i)$ in the trajectory buffer
                \State $\quad$ Set $\mathbf{s}_i \leftarrow \mathbf{s'}_i$
                \State $\quad$ \textbf{if} environment $i$ is done
                \State $\quad$$\quad$Remove environment $i$ from current rollout environments
                \State $\quad$ \textbf{end if}
                \State \textbf{end for}
            \EndFor
            \State $\textbf{Return}$ trajectory buffers
        \end{algorithmic}
        \caption{Asynchronous Trajectory Gathering}
        \label{alg:trajectory}
    \end{algorithm}

    Internally, the learner creates a worker pool before starting the training loop, and the Dealer is then responsible
    for sending each step request asynchronously to each environment Router.
    The environment then waits for either all actions to be received or for a small timeout interval to be reached,
    and returns the results to the learner.
    Since all environments are executed in separate processes/distributed nodes, the learner can continue to process the
    received trajectories while waiting for each asynchronous environment to respond,
    and thus the rollout speed is determined by the speed of the slowest environment.
    This is becomes a significant bottleneck if the environments have different processing speeds,
    but due to the nature of the limit order book simulation, the processing time is, although large,
    still relatively consistent across all environments, and the hard timeout also contributes to minimizing stragglers.

    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{diagrams/pipeline.pdf}
        \caption{Illustration of the pipeline between learners, their workers and a single environment instance.}
        \label{fig:lob_simulation}
    \end{figure}

    In summary, this integrated system architecture—comprising a shared simulation environment, an asynchronous actor–learner framework, and
    a robust communication backbone—addresses many of the limitations inherent in earlier distributed reinforcement learning approaches.
    By combining the advantages of shared resource utilization with scalable, asynchronous updates, the architecture provides a solid foundation for
    developing reinforcement learning systems capable of operating effectively in complex, dynamic domains.
    We implemented both the independent process workers and shared process for learner/workers,
    and decided to use the shared process due to the increased efficiency of running the forward and backward batched passes
    for all environments being faster than the speed gain of running the rollout in parallel.
    That is, instead of gathering the gradients from each worker, we gather the experiences and perform
    a single backward pass for all workers, which we observed to be faster for our use case.

\end{document}