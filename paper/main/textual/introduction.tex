%% USPSC-Introducao.tex

% ----------------------------------------------------------
% Introdução (exemplo de capítulo sem numeração, mas presente no Sumário)
% ----------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}

Over the past decade the field of Reinforcement Learning has seen significant advancements, in a multitude of
different fields, including but not limited to robotics, video games, and finance.
These advancements have been largely driven by the increasing availability of large-scale computing resources, as well as
the development of modern decoupled and distributed architectures, due to the popularity of deep learning techniques.
The Reinforcement Learning (RL) paradigm deals with the problem of learning an optimal
policy through consecutive interactions with an environment, or knowledge of the underlying system dynamics,
aiming to maximize the expected value of its future rewards.

Traditional RL-based frameworks focus on single-agent tasks, and sometimes multi-agent reinforcement learning (MARL),
in which multiple agents act in a shared environment where they can have shared or individual goals, called cooperative or competitive tasks, respectively.
The need for complex decision-making systems further increased the interest in distributing
the learning and acting process of the RL agents, leading to recent developments in distributed RL architectures
which further improved the state-of-the-art in the field.

This has led to the development of distributed RL architectures that aim to parallelize both the simulation of environments and the agent update process itself.
In conventional distributed RL frameworks, multiple agents (also called actors in the context of actor-critic methods) interact with separate instances of the environment,
and aggregate their policy updates using some sort of gradient aggregation mechanism unto a central learner (or critic).
Moreover, the distributed setup addresses computational bottlenecks by decoupling GPU-intensive training from CPU-bound data collection.
Examples of such systems are Ray RLLib and OpenAI's R2D2 which allow decoupling the data collection process (called trajectory generation or collection
in the context of reinforcement learning) from the intensive training phase using centralized learners separated from distributed workers.
This decoupling allows for more efficient use of computational resources, as well as the ability to scale the number of workers and environments independently.

Current state-of-the-art approaches to distributed RL, such as IMPALA and SEED RL, currently optimize
the communication between workers and the learner by means of asynchronous communication with multiple separated environments,
and use the V-trace algorithm to reduce the impact of unstable learning dynamics that might occur from different environment
or worker states, as well as the non-stationarity of the underlying data distributions.<
In many applications, particularly in the context of market making (MM) and high-frequency trading (HFT) environments,
the underlying environment state dynamics have very similar characteristics and distributions, and the use of separate environments
for each agent can lead to redundant computations and increased latency, which do not optimally leverage the parallelism of modern computing systems,
as well as incurring in unnecessary overhead for maintaining entirely separate order arrival processes and redundant computations,
which increase the latency of the system and can lead to worse performance in the context of high-frequency trading.

\section{Challenges in Distributed Reinforcement Learning}
\label{sec:challenges}

Although modern distributed RL systems have shown significant improvements in terms of computational efficiency and learning performance,
several challenges remain when scaling these systems for real-world applications.
One of the main limiting factors is communication overhead, which arises from the latency of transmitting either
gathered trajectories or policy parameter updates between the workers and the centralized learner.
Another common limitation are synchronization delays, which can occur due to different processing speeds
of the workers, called the ``straggler problem'' or from the environment step processing time.
On the other hand, using asynchronous communication can lead to data staleness, where the policy being executed by the workers
can become outdated if workers have not received the latest policy update from the learner.
Suboptimal performance is a natural consequence of data staleness, since the agents
will not be acting according to the current policy, and their updates will be based on either off-policy or on-policy
data, and sometimes both.

Designing a distributed RL system that is successfully able to handle both these problems is a balancing task of
trade-offs between processing power and the overhead of using distributed communication.
The aforementioned V-trace algorithm is an example of a training technique used to stabilize the learning process,
but in the context of high-frequency trading, other problems make the usage of the V-trace algorithm more challenging.

In the context of this work, the unique dynamics of non-stationary and multiple market regimes that show up
due to the presence of latent and exogenous market factors have an increasing impact on the learning process of the agents.
In extreme cases, the changing market regimes coupled with unstable learner updates can violate the Markov property
on which the Bellman equations, described in~\autoref{subsec:environment} and thus break guarantees of convergence for standard RL algorithms,
and approaches toward robust policy learning must be used.

Ensuring a robust and stable learning process under high load conditions requires maintaining a consistent replay buffer
becomes challenging and this issue is especially pronounced in high-frequency,
low-latency domains where minor delays or short periods of staleness lead to significant performance degradation.
Although distributed RL systems offer significant decrease in training time and potential model sizes,
parallelizing both data collection and policy update steps while maintaining stability and scalability remains a major challenge.
The need for data staleness and worker stragglers to be addressed in a way that does not compromise the learning process
under non-stationary market regimes is essential for training RL-based market making agents.


\section{Motivation}
\label{sec:motivation}

In this thesis, we address the challenge of designing a distributed RL system tailored to high-frequency trading environments,
aiming to reduce computational overhead and improve learning performance by minimizing the impact of communication delays and data staleness.
Instead of having individual workers interact with separate environment instances, we adapt the usual multi agent RL paradigm
where multiple agents interact with an asynchronous parallel environment, instead of a single process, single environment setup.
By implementing a shared parallel environment architecture, we aim to reduce the computational overhead of duplicated
simulated environments with similar dynamics and market regimes, aiming to improve resource utilization and reduce the overall
accumulated latency during the training process.
Improving the efficiency of processing power and reducing the communication overhead between the workers and the learner is
especially valuable in high-frequency trading where the simulation speed impacts training turnaround and stability
due to the nature to the underlying market dynamics being closely tied to order arrival, execution and cancelation times.

The proposed architecture is built using a two-tiered design: central learners manage their policies while multiple workers interact with each shared environment
in a parallel and asynchronous fashion.
This also allows for deploying heterogeneous actors within the cluster, but this will only be discussed briefly in this thesis,
as the main focus is rather on the shared environment architecture.

Communication between the learner and workers is handled asynchronously via a brokerless Router/Dealer setup,
minimizing messaging overhead and allowing each worker to send its actions directly to the environment.
As soon as a worker completes a trajectory, the learner is updated, while other workers continue processing without interruption.
This asynchronous design not only improves overall throughput but also enables scaling the learner and the environment independently,
an aspect critical to handling real-time financial data streams.
We use the V-trace algorithm to stabilize learning in the presence of asynchronous updates and non-stationarity,
as proposed in IMPALA distributed RL framework~\cite{WOS:000970111200001}.

By allowing multiple agents to share a single environment, the proposed architecture tries to reduce these redundancies.
This design is particularly attractive for algorithmic trading applications where environments
simulate market conditions and require substantial processing power.
In such cases, sharing an environment can reduce latency and cost while maintaining, or even improving,
the quality of the experience data that feeds into the learning process.

From a practical perspective, distributed RL has been instrumental in tackling problems that were once computationally intractable.
By parallelizing environment interactions and policy updates, these systems achieve better exploration
and faster convergence compared to traditional single-threaded approaches.
but also contributes to more robust policy learning by aggregating diverse trajectories collected from different parts of the state space.
In a financial context, this means that the RL agent can better capture the myriad subtle patterns and anomalies that drive market movements.
Another important aspect motivating this work is the need for resource efficiency.
In standard distributed RL models, the replication of environments across agents leads to considerable overhead in terms of both memory and computational load.

\section{Overview and Contributions}
\label{sec:overview}

In ~\autoref{subsec:pearl} we introduce the details for implementing the Parallel Environments for Asynchronous Reinforcement Learning (PEARL) framework,
and test it using a simulated trading environment that incorporates a realistic limit order book (LOB) mechanism
by using a mixture of stochastic processes to model underlying market dynamics, described in ~\autoref{subsec:environment}
The processes are chosen so as to mimic known stylized facts of financial markets, such as heavy-tailed price returns and
volatility clustering, and are calibrated to match the statistical properties of real-world financial data.

The PEARL architecture is designed to enable multiple agents to interact with a shared environment concurrently and asynchronously,
and we test the training performance of the system using a distributed version of the Proximal Policy Optimization (PPO) algorithm
as the learning agent, described in ~\autoref{sec:agent}.
We evaluate the system's performance in terms of computational efficiency, speedup, learning stability, and convergence speed,
comparing its performance for different numbers of workers and environment nodes.
The results in ~\autoref{sec:results} demonstrate that the shared environment model can achieve superior computational efficiency and learning performance compared to
conventional multi-agent reinforcement learning frameworks (MARL), such as IMPALA and R2D2.
