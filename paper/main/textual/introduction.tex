%% USPSC-Introducao.tex

% ----------------------------------------------------------
% Introdução (exemplo de capítulo sem numeração, mas presente no Sumário)
% ----------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}

The increasing availability of large-scale computing infrastructure has driven significant advances
in Reinforcement Learning (RL) across several application domains, such as robotics, video games, and finance.
In parallel, the widespread adoption of deep learning methods has fostered the development of scalable distributed architectures,
enabling RL agents to train in complex environments with improved data throughput and convergence rates.
The RL paradigm formalizes sequential decision-making as the process of learning a policy that maximizes the expected return via interaction with an environment,
either directly or through a known model of its dynamics.

To scale RL to high-dimensional or computationally expensive settings, the standard approach has evolved from single-agent
training toward distributed setups that decouple experience collection from policy optimization.
This is commonly achieved by parallelizing the simulation of environments and distributing agent interactions across multiple workers,
referred to as actors in the context of actor-critic methods.
These actors interact with separate environment instances and asynchronously send data to a central learner,
which updates the policy using aggregated gradients.
This setup allows for efficient resource allocation by separating the CPU-bound data collection phase from the GPU-intensive training phase.
Notable frameworks employing this structure include Ray RLLib, OpenAI’s R2D2, and IMPALA,
which introduced the V-trace algorithm to stabilize learning under stale data conditions and asynchronous communication.

Despite their performance gains, conventional distributed RL architectures exhibit critical inefficiencies when
deployed in domains where the environment dynamics are highly similar across agents.
High-frequency trading (HFT) is a paradigmatic case of this scenario: market making environments typically share nearly
identical state transition characteristics across simulations.
As a result, using separate environment instances per agent leads to redundant computations, duplicated order flow modeling, and unnecessary latency overhead.
In latency-sensitive settings such as algorithmic trading, these inefficiencies directly degrade policy quality and responsiveness.
Furthermore, communication delays, data staleness, and the presence of straggler workers contribute to instability during training,
compounding the difficulties in environments governed by non-stationary market regimes.

This thesis introduces a distributed RL architecture specifically designed to address these limitations in the context of high-frequency financial environments.
By replacing independent environment replicas with a shared, parallelized environment structure,
the proposed system reduces computational redundancy and allows agents to asynchronously interact with a common market simulator.
The architecture separates policy learning and trajectory collection while eliminating the overhead incurred by reducing computations on similar market dynamics.
This design leverages the fact that in realistic market simulation, the underlying dynamics
---such as order arrivals and cancellations---can be modeled centrally without requiring independent instantiations per agent.

In what follows, we examine the primary challenges of distributed RL under non-stationary and asynchronous conditions,
identify the architectural limitations of existing systems in latency-critical domains, and motivate the need for a shared environment structure.
The system proposed herein, PEARL (Parallel Environments for Asynchronous Reinforcement Learning),
implements this approach using a brokerless asynchronous messaging layer and an environment model calibrated to reproduce stylized facts of limit order books.
We evaluate the system’s performance using a distributed PPO-based learner and demonstrate improvements in
throughput, convergence, and learning stability over conventional MARL baselines.

\section{Challenges in Distributed Reinforcement Learning}
\label{sec:challenges}

Despite their effectiveness in accelerating policy learning and scaling reinforcement learning to high-dimensional tasks,
distributed RL architectures face several inherent challenges that hinder their deployment in real-time, non-stationary environments.
Chief among these are communication overhead and synchronization inefficiencies.
Communication overhead arises from the continuous exchange of trajectories or gradient updates between the distributed workers and the centralized learner,
which introduces latency and reduces throughput.
Synchronization delays, often caused by heterogeneous processing times among workers, a phenomenon referred to as the ``straggler problem'',
further exacerbate inefficiencies in synchronous training pipelines.

To mitigate these bottlenecks, many distributed RL frameworks adopt asynchronous communication schemes.
However, this design introduces a different class of instability related to data staleness:
workers may act on outdated policies and contribute updates based on trajectories collected under older versions of the policy.
This leads to inconsistencies in the learning signal, particularly in algorithms that rely on on-policy assumptions.
In highly dynamic environments, such staleness can result in suboptimal or divergent learning behavior, especially when policy updates are applied to non-stationary data distributions.

This trade-off between minimizing communication overhead and preserving the timeliness of policy synchronization is a central design challenge in distributed RL.
The V-trace algorithm partially addresses this issue by correcting off-policy trajectories under a truncated importance sampling regime,
stabilizing the policy gradient under asynchronous updates.
While effective in many domains, its application in financial contexts such as high-frequency trading is more delicate.

In HFT environments, the agent operates under volatile and evolving market regimes shaped by latent and exogenous factors.
These regime shifts alter the underlying reward and transition dynamics, which can violate the Markov property assumed by the Bellman operator.
In such cases, the environment dynamics may no longer be sufficiently captured by a stationary transition kernel,
undermining the theoretical convergence guarantees of classical RL algorithms.
Learners may exhibit high sensitivity to delayed updates or incorrectly prioritized experiences in the replay buffer, making robust policy learning significantly more difficult.

Moreover, maintaining a consistent and representative replay buffer becomes non-trivial in low-latency domains.
Slight inconsistencies or delays in trajectory ingestion can amplify instability,
particularly when the temporal precision of events such as order placements or cancellations is critical to agent performance.
Data staleness and worker lag, if not explicitly accounted for, lead to an erosion of policy quality and increased training variance.
This is especially problematic under asynchronous actor-learner configurations where the buffer can be populated with trajectories misaligned with the current market regime.

Although distributed RL reduces overall training time and supports scaling to large actor populations,
the joint requirement of stability, low latency, and robustness to regime shifts imposes stricter design constraints.
Addressing these limitations without sacrificing the convergence properties of the learning algorithm is essential
for the reliable deployment of RL-based decision systems in financial markets.

\section{Motivation and Proposed Solution}
\label{sec:motivation}

This work proposes a distributed reinforcement learning architecture specifically designed for high-frequency trading (HFT) environments,
where both computational efficiency and synchronization precision are critical.
The proposed system addresses two primary limitations observed in conventional distributed RL frameworks:
the computational burden of replicating multiple independent environment instances and the instability caused by
policy lag and data staleness in asynchronous setups.

To address these, we introduce a shared parallel environment model in which multiple agents interact concurrently with a single,
centralized environment server.
Unlike traditional architectures that assign one environment per worker process, our approach aligns more closely with the multi-agent RL paradigm,
where multiple actors cohabit the same environment instance while preserving asynchronous interactions.
This shared model eliminates the redundancy of simulating nearly identical market conditions across many replicated environments,
thereby reducing both memory usage and CPU scheduling overhead.
Furthermore, by centralizing environment dynamics, we preserve consistency in regime transitions and exogenous event modeling,
which is particularly important in simulating realistic market microstructure behavior.

A key design feature is the system’s two-tiered architecture: decentralized actors communicate asynchronously
with the environment and submit their trajectory rollouts directly to a centralized learner.
Communication is implemented using a brokerless Router/Dealer pattern, which minimizes latency by
removing intermediaries between workers and a centralized message queue.
Each actor issues actions independently and receives environment responses without blocking,
while completed trajectories are streamed asynchronously to the learner, limited only by a configurable timeout rate.
This architecture supports high-throughput, low-latency learning by avoiding synchronization barriers and
allowing workers to operate at their own pace—a critical property in heterogeneous or bursty market regimes.

Policy updates are managed asynchronously by the learner, and to maintain stability under off-policy drift,
we employ the V-trace correction mechanism.
V-trace provides truncated importance sampling-based corrections for trajectories collected under older policies,
enabling learning to remain stable even when the actor population is not uniformly synchronized.
This choice is particularly suited to financial environments, where policy iteration must remain robust despite high-frequency,
non-stationary signal streams and abrupt regime shifts.

The architecture is inherently modular, enabling scalability along both the actor and learner axes.
Workers can be dynamically added to increase throughput,
and multiple learners can be instantiated in parallel when using ensemble or heterogeneous policies.
While this work focuses on homogeneous policy learning, the architecture naturally supports multi-policy training setups.
Furthermore, the design makes it possible to simulate temporally aligned interactions among
agents—crucial in modeling order book dynamics and adverse selection—without introducing artificial latency or violating temporal causality.

In contrast to prior work that emphasizes scaling via massive actor populations,
our system emphasizes resource efficiency and realism considering computational constraints of quantitative trading funds.
By consolidating market dynamics into a shared asynchronous environment,
we aimed to reduce the computational load without compromising the statistical diversity or quality of collected experiences.
This enables more frequent learner updates with fresher data, and improves convergence rates by limiting the staleness gap
between the policy used for data collection and the policy being updated.

Overall, the proposed architecture aims to balance scalability, learning stability, and environmental fidelity,
all of which are necessary for deploying reinforcement learning agents in high-frequency trading systems.
By explicitly addressing the trade-offs between communication latency, synchronization lag, and policy consistency,
this system serves as a step toward robust, scalable, and economically viable RL systems in financial domains.

\section{Overview and Contributions}
\label{sec:overview}

In ~\autoref{subsec:pearl} we introduce the details for implementing the Parallel Environments for Asynchronous Reinforcement Learning (PEARL) framework,
and test it using a simulated trading environment that incorporates a realistic limit order book (LOB) mechanism
by using a mixture of stochastic processes to model underlying market dynamics, described in ~\autoref{subsec:environment}
The processes are chosen so as to mimic known stylized facts of financial markets, such as heavy-tailed price returns and
volatility clustering, and are calibrated to match the statistical properties of real-world financial data.

The PEARL architecture is designed to enable multiple agents to interact with a shared environment concurrently and asynchronously,
and we test the training performance of the system using a distributed version of the Proximal Policy Optimization (PPO) algorithm
as the learning agent, described in ~\autoref{sec:agent}.
We evaluate the system's performance in terms of computational efficiency, speedup, learning stability, and convergence speed,
comparing its performance for different numbers of workers and environment nodes.
The results in ~\autoref{sec:results} demonstrate that the shared environment model
achieved superior computational efficiency and learning performance compared to
conventional multi-agent reinforcement learning frameworks (MARL), such as IMPALA and R2D2 for the same environment.
