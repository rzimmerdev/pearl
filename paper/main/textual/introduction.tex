%% USPSC-Introducao.tex

% ----------------------------------------------------------
% Introdução (exemplo de capítulo sem numeração, mas presente no Sumário)
% ----------------------------------------------------------
\chapter{Introduction}
\label{ch:introduction}

The increasing availability of large-scale computing infrastructure has driven significant advances
in Reinforcement Learning (RL) across several application domains, such as robotics, video games, and finance.
In parallel, the widespread adoption of deep learning methods has fostered the development of scalable distributed architectures,
enabling RL agents to train in complex environments with improved data throughput and convergence rates.
The RL paradigm formalizes sequential decision-making as the process of learning a policy that maximizes the expected return via interaction with an environment,
either directly or through a known model of its dynamics.

To scale RL to high-dimensional or computationally expensive settings, the standard approach has evolved from single-agent
training toward distributed setups that decouple experience collection from policy optimization.
This is commonly achieved by parallelizing the simulation of environments and distributing agent interactions across multiple workers,
referred to as actors in the context of actor-critic methods.
These actors interact with separate environment instances and asynchronously send data to a central learner,
which in turn updates the centralized policy through aggregate gradient updates, allowing for decentralized resource allocation during training.
This setup allows for efficient resource allocation by separating the CPU-bound data collection phase from the GPU-intensive training phase.

Despite their performance gains, conventional distributed RL architectures exhibit critical inefficiencies when
deployed in domains where the environment dynamics are highly similar across agents.
High-frequency trading (HFT) is a paradigmatic case of this scenario: market making environments typically share nearly
identical state transition characteristics across simulations.
As a result, using separate environment instances per agent leads to redundant computations, duplicated order flow modeling, and unnecessary latency overhead.
In latency-sensitive settings such as algorithmic trading, these inefficiencies directly degrade policy quality and responsiveness.
Furthermore, communication delays, data staleness, and the presence of straggler workers contribute to instability during training,
compounding the difficulties in environments governed by non-stationary market regimes.

This thesis introduces a distributed RL architecture specifically designed to address these limitations in the context of high-frequency financial environments.
By replacing independent environment replicas with a shared, parallelized environment structure,
the proposed system reduces computational redundancy and allows agents to asynchronously interact with a common market simulator.
The architecture separates policy learning and trajectory collection while eliminating the overhead incurred by reducing computations on similar market dynamics.
This design leverages the fact that in realistic market simulation, the underlying dynamics
---such as order arrivals and cancellations---can be modeled centrally without requiring independent instantiations per agent.

In what follows, we examine the primary challenges of distributed RL under non-stationary and asynchronous conditions,
identify the architectural limitations of existing systems in latency-critical domains, and motivate the need for a shared environment structure.
The system proposed herein, PEARL (Parallel Environments for Asynchronous Reinforcement Learning),
implements this approach using a brokerless asynchronous messaging layer and an environment model calibrated to reproduce stylized facts of limit order books.
We evaluate the system's performance using a distributed PPO-based learner and demonstrate improvements in
throughput, convergence, and learning stability over conventional MARL baselines.

\section{Challenges in Distributed Reinforcement Learning}
\label{sec:challenges}

Even though distributed architectures facilitate policy learning and allow for scaling the training of RL strategies for
high-dimensional tasks, they still present inherent challenges when deployed in real-time, non-stationary environments.
Among the most impactful inefficiencies lies communication overhead and synchronization delays.
Regarding decentralized RL training loops, communication overhead arises from exchanging either trajectory paths or gradient updates
between workers and learners, introducing latency and thus reducing throughput rates.
Synchronization delays, often caused by heterogeneous processing times among workers, a phenomenon referred to as the ``straggler problem'',
lead to inefficiencies in the training pipeline.

Additionally, in HFT environments, the agent operates under volatile and evolving market regimes shaped by latent and exogenous factors.
Regime shifts also alter the underlying reward and transition dynamics, which can violate the Markov property assumed by the Bellman operator.
In such cases, the environment dynamics may no longer be sufficiently captured by a stationary transition kernel,
undermining the theoretical convergence guarantees of classical RL algorithms.
Learners may exhibit high sensitivity to delayed updates or incorrectly prioritized experiences in the replay buffer,
making robust policy learning significantly more difficult.

To mitigate these bottlenecks, many distributed RL frameworks adopt asynchronous communication schemes both
to improve training speeds by increasing sample efficiency, and to reduce training instability and increase convergence chances.
However, the asynchronous design introduces a different class of instability related to data staleness:
workers may act on outdated policies and contribute updates based on trajectories collected under older versions of the policy.
When outdated trajectories are used to update algorithms that rely on on-policy assumptions,
especially in highly dynamic and non-stationary environments,
suboptimal policy changes learn to undesired effects on the reward curve and sometimes even increase chances of divergence during training.

Thus, a trade-off between minimizing communication overhead and preserving the timeliness of policy synchronization
becomes a centerpiece consideration when designing distributed RL algorithms.
An algorithm that addresses this trade-off is the V-trace algorithm, proposed in the Impala framework,
which corrects off-policy trajectories by means of a truncated importance sapling regimes,
reducing data the effects of data staleness on the asynchronous policy gradient updates.
While effective both in finance and other domains, its application in high-frequency contexts can be more delicate
and require additional consideration due to the additional processing delays it adds as well as the requirement for a
centralized learner.

Moreover, maintaining a consistent and representative replay buffer becomes non-trivial in low-latency domains.
Slight inconsistencies or delays in the trajectory gathering steps can amplify instability,
particularly when the timestep of each event, such as order placements or cancellations, is critical to agent performance.
Data staleness and worker lag, if not explicitly accounted for, lead to an erosion of policy quality and increased training variance.
This is specially problematic under asynchronous actor-learner configurations where the buffer can be populated with trajectories misaligned with the current market regime.

Although distributed RL reduces overall training time and supports scaling to large actor populations,
the joint requirement of stability, low latency, and robustness to regime shifts imposes stricter design constraints.
Addressing these limitations without sacrificing the convergence properties of the learning algorithm is essential
for the reliable deployment of RL-based decision systems in financial markets.

\section{Motivation and Proposed Solution}
\label{sec:motivation}

In this thesis we propose a distributed algorithm for training reinforcemnet learning agents on high-frequency trading environments.
The proposed system addresses two primary limitations observed in such environments:
the computational burden of replicating multiple independent environment instances and the instability caused by
policy lag and data staleness when training is performed asynchronously.

Our algorithm is based on a shared parallel environment model, where the underlying dynamics are simulated
through a mixture of stochastic processes from which random order events are sampled and inserted into a red-black tree limit order book.
Unlike traditional architectures that assign one environment per worker process,
our approach aligns more closely with the underlying design of market-based matching,
where multiple buyers and sellers can simultaneously send orders, on which financial exchanges are constructed upon.
This shared model eliminates the redundancy of simulating nearly identical market conditions across many replicated environments,
thereby reducing both memory usage and CPU scheduling overhead.
Furthermore, by sharing environment dynamics, multiple agents observe similar regime transitions and share exogenous event observations,
which is particularly important in simulating realistic market microstructure behavior
\footnote{Although the current state of the art in exogenous event modeling is achieved through world models or agents,
we decided to not use world agents as this would go outside the scope of this work.}.

A key design feature of the algorithm is the decoupled worker-environment architecture:
we separate the workers from the environments by inserting a communication layer for trajectory passing.
Communication is implemented using a brokerless Router/Dealer pattern, aiming to minimize latency by
removing possible bottlenecks of centralized message queues.
Each actor issues actions independently and receives non-blocking environment responses,
while completed trajectories are streamed asynchronously to the learner,
limited only by a configurable timeout rate to reduce chances of stragglers.
This architecture supports high-throughput, low-latency learning by avoiding synchronization barriers and
allowing workers to operate at their own pace---a critical property when the environment's regimes drift and
thus resulting in heterogeneous processing times.

As long as enough processing cores exist, workers can be added to increase throughput, as long
as sufficiently enough environments exist to accomodate the created workers.
While this work focuses on homogeneous policy learning, the architecture also naturally supports multi-policy training setups.
In contrast to prior work that emphasizes scaling via massive actor populations,
our system emphasizes resource efficiency and realism by additionally considering computational constraints of small quantitative trading funds.
By consolidating market dynamics into a shared asynchronous environment,
we aim to reduce the computational load without compromising the statistical diversity or quality of collected experiences.
This enables more frequent learner updates with fresher data, and improves convergence rates by limiting the staleness gap
between the policy used for data collection and the policy being updated.

Overall, the proposed architecture aims to balance scalability and learning stability,
which are necessary for both training reinforcement learning agents and fine-tuning them to daily changes
in market rates, such as expected return and volatity.
By explicitly addressing the trade-offs between communication latency, synchronization lag, and stability,
this system serves as a step toward scalable and time viable RL systems in financial domains.

\section{Overview and Contributions}
\label{sec:overview}

In ~\autoref{subsec:pearl} we introduce the details for implementing the Parallel Environments for Asynchronous Reinforcement Learning (PEARL) framework,
and test it using a simulated trading environment that incorporates a realistic limit order book (LOB) mechanism
by using a mixture of stochastic processes to model underlying market dynamics, described in ~\autoref{subsec:environment}
The processes are chosen so as to mimic known stylized facts of financial markets, such as heavy-tailed price returns and
volatility clustering, and are calibrated to match the statistical properties of real-world financial data.

The PEARL architecture is designed to enable multiple agents to interact with a shared environment concurrently and asynchronously,
and we test the training performance of the system using a distributed version of the Proximal Policy Optimization (PPO) algorithm
as the learning agent, described in ~\autoref{sec:agent}.
We evaluate the system's performance in terms of computational efficiency, speedup, learning stability, and convergence speed,
comparing its performance for different numbers of workers and environment nodes.
The results in ~\autoref{sec:results} demonstrate that the shared environment model
achieved superior computational efficiency and learning performance compared to
conventional multi-agent reinforcement learning frameworks (MARL), such as IMPALA and R2D2 for the same environment.
